"""
Training and evaluation runners for InforMARL
"""
import numpy as np
import time
from typing import List, Dict, Any

from ..env import BottleneckInforMARLEnv
from ..env.graph_builder import build_graph_observations
from ..env.vec_env import VectorizedBottleneckEnv
from ..utils.device import clear_gpu_memory, get_memory_usage


def train_agents_parallel(env: BottleneckInforMARLEnv, vec_env: VectorizedBottleneckEnv, 
                         num_episodes: int = 100) -> List[float]:
    """ë³‘ë ¬ ê²½í—˜ ìˆ˜ì§‘ì„ í™œìš©í•œ í•™ìŠµ í•¨ìˆ˜"""
    episode_rewards = []
    total_steps = 0
    # performance.yamlì—ì„œ steps_per_worker ì‚¬ìš©
    steps_per_collection = 25  # ê¸°ë³¸ê°’
    # ì¼ë‹¨ ê¸°ë³¸ê°’ ì‚¬ìš© (ì¶”í›„ performance ì„¤ì • ì „ë‹¬ ê°€ëŠ¥)
    
    print(f"=== ë³‘ë ¬ í•™ìŠµ ì‹œì‘: {vec_env.num_workers}ê°œ ì›Œì»¤ ì‚¬ìš© ===")
    
    try:
        for episode in range(num_episodes):
            episode_start_time = time.time()
            
            # ğŸš€ ë³‘ë ¬ë¡œ ê²½í—˜ ìˆ˜ì§‘ (4ë°° ë¹ ë¦„!)
            print(f"Episode {episode}: ë³‘ë ¬ ê²½í—˜ ìˆ˜ì§‘ ì‹œì‘...")
            all_experiences, worker_infos = vec_env.collect_parallel_experiences(steps_per_collection)
            
            # ìˆ˜ì§‘ëœ ê²½í—˜ í†µê³„ ìˆ˜ì •
            total_rewards = 0
            for exp in all_experiences:
                rewards = exp['rewards']
                if isinstance(rewards, list):
                    total_rewards += sum(rewards)
                else:
                    total_rewards += rewards if rewards else 0
            
            print(f"  ë””ë²„ê·¸: ì „ì²´ ê²½í—˜ {len(all_experiences)}ê°œ, ì´ ë³´ìƒ {total_rewards:.2f}")
            if len(all_experiences) > 0:
                print(f"  ë””ë²„ê·¸: ì²« ë²ˆì§¸ ê²½í—˜ ë³´ìƒ: {all_experiences[0]['rewards']}")
            
            episode_rewards.append(total_rewards)
            total_steps += len(all_experiences)
            
            # ğŸš€ ìˆ˜ì§‘ëœ ê²½í—˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ (ë©”ì¸ í™˜ê²½ ì‚¬ìš©)
            print(f"  ìˆ˜ì§‘ëœ ê²½í—˜: {len(all_experiences)}ê°œ, ë³´ìƒí•©: {total_rewards:.2f}")
            
            # ë””ë²„ê¹…: ë³´ìƒì´ 0ì´ë©´ ì›ì¸ ì¶”ì 
            if total_rewards == 0 and len(all_experiences) > 0:
                print(f"  ê²½ê³ : ë³´ìƒì´ 0ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ ê²½í—˜ ë³´ìƒ: {all_experiences[0]['rewards']}")
                print(f"  ì²« ë²ˆì§¸ ê²½í—˜ info: {all_experiences[0]['info']}")
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ëŠ” ë³´ìƒì´ ìˆì„ ë•Œë§Œ ìˆ˜í–‰
            if total_rewards != 0:
                env._update_shared_networks()
                if hasattr(env, 'gnn_optimizer') and env.gnn_optimizer is not None:
                    env.gnn_optimizer.step()
                    env.gnn_optimizer.zero_grad()
            else:
                print("  ë³´ìƒì´ 0ì´ë¯€ë¡œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
            
            # train.yamlì—ì„œ eval_frequency ì‚¬ìš©
            eval_freq = 5  # ê¸°ë³¸ê°’
            if hasattr(env, 'train_config') and 'training' in env.train_config:
                eval_freq = env.train_config['training'].get('eval_frequency', 5)
            
            if episode % eval_freq == 0:
                avg_reward = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)
                episode_time = time.time() - episode_start_time
                
                # ì›Œì»¤ë³„ ì„±ê³µë¥  ìˆ˜ì§‘
                success_rates = [info['final_info'].get('success_rate', 0.0) for info in worker_infos]
                avg_success_rate = np.mean(success_rates) if success_rates else 0.0
                
                memory_info = get_memory_usage()
                print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, "
                      f"Success Rate = {avg_success_rate:.2f}, "
                      f"Time = {episode_time:.2f}s, Memory: {memory_info}")
            
            # performance.yamlì—ì„œ clear_memory_interval ì‚¬ìš©
            clear_interval = 20  # ê¸°ë³¸ê°’
            # ì¼ë‹¨ ìš°ì„  ê¸°ë³¸ê°’ ì‚¬ìš© (ì¶”í›„ ê°œì„  ê°€ëŠ¥)
            if episode % clear_interval == 0 and episode > 0:
                clear_gpu_memory()
    
    except KeyboardInterrupt:
        print("\ní•™ìŠµ ì¤‘ë‹¨ë¨")
    finally:
        vec_env.close()
    
    return episode_rewards


def train_agents(env: BottleneckInforMARLEnv, num_episodes: int = 100) -> List[float]:
    """ê¸°ì¡´ ë‹¨ì¼ í™˜ê²½ í•™ìŠµ í•¨ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€)"""
    episode_rewards = []
    
    for episode in range(num_episodes):
        observations = env.reset()
        episode_reward = 0
        
        for step in range(env.max_timesteps):
            observations, rewards, done, info = env.step()
            episode_reward += sum(rewards)
            
            # train.yamlì—ì„œ update_frequency ì‚¬ìš©
            update_freq = 25  # ê¸°ë³¸ê°’
            if hasattr(env, 'train_config') and 'training' in env.train_config:
                update_freq = env.train_config['training'].get('update_frequency', 25)
            
            if step % update_freq == 0:
                # ğŸš€ ê³µìœ  ë„¤íŠ¸ì›Œí¬ ë°°ì¹˜ ì—…ë°ì´íŠ¸ (ê°œë³„ ì—…ë°ì´íŠ¸ ëŒ€ì‹ )
                env._update_shared_networks()
                # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                if hasattr(env, 'gnn_optimizer') and env.gnn_optimizer is not None:
                    env.gnn_optimizer.step()
                    env.gnn_optimizer.zero_grad()
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        
        # ì—í”¼ì†Œë“œ ëì— í•œ ë²ˆ ë” ì—…ë°ì´íŠ¸
        env._update_shared_networks()
        # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        if hasattr(env, 'gnn_optimizer') and env.gnn_optimizer is not None:
            env.gnn_optimizer.step()
            env.gnn_optimizer.zero_grad()
            
        if episode % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            memory_info = get_memory_usage()
            print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, "
                  f"Success Rate = {info['success_rate']:.2f}, Memory: {memory_info}")
        
        # train.yamlì—ì„œ ì„¤ì •ëœ ë¹ˆë„ë¡œ GPU ë©”ëª¨ë¦¬ ì •ë¦¬  
        clear_interval = 20  # ê¸°ë³¸ê°’
        if episode % clear_interval == 0 and episode > 0:
            clear_gpu_memory()
    
    return episode_rewards


def evaluate_with_animation(env: BottleneckInforMARLEnv, num_episodes: int = 5, 
                          render_delay: float = 0.2) -> List[float]:
    """í‰ê°€ ëª¨ë“œë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰í•˜ë©° ì• ë‹ˆë©”ì´ì…˜ í‘œì‹œ"""
    print("=== InforMARL í‰ê°€ ëª¨ë“œ (ì• ë‹ˆë©”ì´ì…˜) ===")
    print("ì°½ì´ ì—´ë¦¬ë©´ ì—ì´ì „íŠ¸ ì›€ì§ì„ì„ ê´€ì°°í•˜ì„¸ìš”!")
    
    episode_rewards = []
    
    for episode in range(num_episodes):
        print(f"\nì—í”¼ì†Œë“œ {episode + 1}/{num_episodes} ì‹œì‘")
        observations = env.reset()
        episode_reward = 0
        
        # ì´ˆê¸° ìƒíƒœ ë Œë”ë§
        env.render()
        time.sleep(render_delay * 2)  # ì´ˆê¸° ìƒíƒœ ì¢€ ë” ì˜¤ë˜ ë³´ì—¬ì£¼ê¸°
        
        for step in range(env.max_timesteps):
            # í‰ê°€ ëª¨ë“œë¡œ í–‰ë™ ì„ íƒ (training=False)
            actions, _, _ = env._get_batch_actions(
                build_graph_observations(env.agents, env.landmarks, env.obstacles, env.sensing_radius), 
                training=False
            )
            
            # í•œ ìŠ¤í… ì‹¤í–‰
            observations, rewards, done, info = env.step(actions)
            episode_reward += sum(rewards)
            
            # ë Œë”ë§
            env.render()
            
            # ì›€ì§ì„ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê·¸ ì¶œë ¥
            if step % 20 == 0:
                print(f"    ìŠ¤í… {step}: ì—ì´ì „íŠ¸ ìœ„ì¹˜ë“¤")
                for i, agent in enumerate(env.agents):
                    print(f"      Agent {i}: ({agent.x:.1f}, {agent.y:.1f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
            
            time.sleep(render_delay)
            
            if done:
                print(f"  ì—í”¼ì†Œë“œ ì™„ë£Œ! ìŠ¤í…: {step + 1}")
                break
        
        episode_rewards.append(episode_reward)
        print(f"  ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.2f}")
        print(f"  ì„±ê³µë¥ : {info['success_rate']:.2f}")
        print(f"  ì¶©ëŒ íšŸìˆ˜: {info['collision_count']}")
        
        # ì—í”¼ì†Œë“œ ê°„ ì ì‹œ ëŒ€ê¸°
        print("  ë‹¤ìŒ ì—í”¼ì†Œë“œê¹Œì§€ ì ì‹œ ëŒ€ê¸°...")
        time.sleep(2.0)
    
    avg_reward = np.mean(episode_rewards)
    print(f"\n=== í‰ê°€ ê²°ê³¼ ===")
    print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {avg_reward:.3f}")
    
    return episode_rewards


def run_informarl_experiment(num_episodes: int = 100, num_agents: int = 6, config: dict = None, train_config: dict = None, gpu_id: int = None, force_cpu: bool = False, use_parallel: bool = True, num_workers: int = 4) -> tuple[List[float], BottleneckInforMARLEnv]:
    """InforMARL ì‹¤í—˜ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€)"""
    print("=== InforMARL 2D ë³‘ëª© í™˜ê²½ í•™ìŠµ ì‹œì‘ ===")
    
    # ë©”ì¸ í™˜ê²½ ìƒì„± (í•™ìŠµìš© ë„¤íŠ¸ì›Œí¬ ë³´ìœ )
    main_env = BottleneckInforMARLEnv(num_agents=num_agents, config=config, train_config=train_config, gpu_id=gpu_id, force_cpu=force_cpu)
    
    if use_parallel and num_workers > 1:
        print(f"\nğŸš€ ë³‘ë ¬ ëª¨ë“œ: {num_workers}ê°œ ì›Œì»¤ ì‚¬ìš©")
        
        # ë³‘ë ¬ í™˜ê²½ ì„¤ì •
        env_config = {
            'num_agents': num_agents,
            'config': config,
            'train_config': train_config,  # train.yaml ì„¤ì • ì „ë‹¬
            'gpu_id': None,  # ì›Œì»¤ëŠ” CPU ì‚¬ìš©
            'force_cpu': True  # ì›Œì»¤ëŠ” CPUë¡œ ê°•ì œ
        }
        
        vec_env = VectorizedBottleneckEnv(env_config, num_workers=num_workers)
        episode_rewards = train_agents_parallel(main_env, vec_env, num_episodes=num_episodes)
    else:
        print("\nì¼ë°˜ ëª¨ë“œ: ë‹¨ì¼ í™˜ê²½ ì‚¬ìš©")
        episode_rewards = train_agents(main_env, num_episodes=num_episodes)
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
    print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {np.mean(episode_rewards):.3f}")
    
    return episode_rewards, main_env


def run_animation_demo(num_agents: int = 4) -> List[float]:
    """ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ì‹¤í–‰"""
    print("=== InforMARL ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents)
    
    # ê°„ë‹¨í•œ í•™ìŠµ (ì„ íƒì‚¬í•­)
    print("ê°„ë‹¨í•œ ì‚¬ì „ í•™ìŠµ ì¤‘...")
    train_agents(env, num_episodes=10)
    
    # ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ í‰ê°€
    print("\ní‰ê°€ ëª¨ë“œ ì• ë‹ˆë©”ì´ì…˜ ì‹œì‘!")
    results = evaluate_with_animation(env, num_episodes=3, render_delay=0.2)
    
    return results


def quick_movement_test(num_agents: int = 2) -> bool:
    """ì—ì´ì „íŠ¸ ì›€ì§ì„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print("=== ì—ì´ì „íŠ¸ ì›€ì§ì„ í…ŒìŠ¤íŠ¸ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents, max_timesteps=50)
    observations = env.reset()
    
    print("ì´ˆê¸° ìƒíƒœ ë Œë”ë§...")
    env.render()
    time.sleep(1)
    
    for step in range(300):
        # ëœë¤ í–‰ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        random_actions = [np.random.randint(0, 4) for _ in range(num_agents)]
        
        print(f"ìŠ¤í… {step}: í–‰ë™ {random_actions}")
        observations, rewards, done, info = env.step(random_actions)
        
        # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ì¶œë ¥
        for i, agent in enumerate(env.agents):
            print(f"  Agent {i}: ({agent.x:.2f}, {agent.y:.2f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
        
        env.render()
        time.sleep(0.5)
        
        if done:
            break
    
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    import matplotlib.pyplot as plt
    plt.show()
    return True