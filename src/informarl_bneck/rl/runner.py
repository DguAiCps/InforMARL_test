"""
Training and evaluation runners for InforMARL
"""
import numpy as np
import time
from typing import List, Dict, Any

from ..env import BottleneckInforMARLEnv
from ..env.graph_builder import build_graph_observations
from ..utils.device import clear_gpu_memory, get_memory_usage


def train_agents(env: BottleneckInforMARLEnv, num_episodes: int = 100) -> List[float]:
    """ì‹¤ì œ í•™ìŠµì´ í¬í•¨ëœ í•¨ìˆ˜"""
    episode_rewards = []
    
    for episode in range(num_episodes):
        observations = env.reset()
        episode_reward = 0
        
        for step in range(env.max_timesteps):
            observations, rewards, done, info = env.step()
            episode_reward += sum(rewards)
            
            # ë§¤ NìŠ¤í…ë§ˆë‹¤ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ì„±ëŠ¥ ìµœì í™”: 10 â†’ 50)
            if step % 50 == 0:
                for agent in env.informarl_agents:
                    agent.update_networks(env.shared_gnn)
                # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                env.gnn_optimizer.step()
                env.gnn_optimizer.zero_grad()
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        
        # ì—í”¼ì†Œë“œ ëì— í•œ ë²ˆ ë” ì—…ë°ì´íŠ¸
        for agent in env.informarl_agents:
            agent.update_networks(env.shared_gnn)
        # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
        env.gnn_optimizer.step()
        env.gnn_optimizer.zero_grad()
            
        if episode % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            memory_info = get_memory_usage()
            print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, "
                  f"Success Rate = {info['success_rate']:.2f}, Memory: {memory_info}")
        
        # ğŸš€ ì„œë²„ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬: ë§¤ 20 ì—í”¼ì†Œë“œë§ˆë‹¤ ìºì‹œ ì •ë¦¬
        if episode % 20 == 0 and episode > 0:
            clear_gpu_memory()
    
    return episode_rewards


def evaluate_with_animation(env: BottleneckInforMARLEnv, num_episodes: int = 5, 
                          render_delay: float = 0.2) -> List[float]:
    """í‰ê°€ ëª¨ë“œë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰í•˜ë©° ì• ë‹ˆë©”ì´ì…˜ í‘œì‹œ"""
    print("=== InforMARL í‰ê°€ ëª¨ë“œ (ì• ë‹ˆë©”ì´ì…˜) ===")
    print("ì°½ì´ ì—´ë¦¬ë©´ ì—ì´ì „íŠ¸ ì›€ì§ì„ì„ ê´€ì°°í•˜ì„¸ìš”!")
    
    episode_rewards = []
    
    for episode in range(num_episodes):
        print(f"\nì—í”¼ì†Œë“œ {episode + 1}/{num_episodes} ì‹œì‘")
        observations = env.reset()
        episode_reward = 0
        
        # ì´ˆê¸° ìƒíƒœ ë Œë”ë§
        env.render()
        time.sleep(render_delay * 2)  # ì´ˆê¸° ìƒíƒœ ì¢€ ë” ì˜¤ë˜ ë³´ì—¬ì£¼ê¸°
        
        for step in range(env.max_timesteps):
            # í‰ê°€ ëª¨ë“œë¡œ í–‰ë™ ì„ íƒ (training=False)
            actions, _, _ = env._get_batch_actions(
                build_graph_observations(env.agents, env.landmarks, env.obstacles, env.sensing_radius), 
                training=False
            )
            
            # í•œ ìŠ¤í… ì‹¤í–‰
            observations, rewards, done, info = env.step(actions)
            episode_reward += sum(rewards)
            
            # ë Œë”ë§
            env.render()
            
            # ì›€ì§ì„ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê·¸ ì¶œë ¥
            if step % 20 == 0:
                print(f"    ìŠ¤í… {step}: ì—ì´ì „íŠ¸ ìœ„ì¹˜ë“¤")
                for i, agent in enumerate(env.agents):
                    print(f"      Agent {i}: ({agent.x:.1f}, {agent.y:.1f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
            
            time.sleep(render_delay)
            
            if done:
                print(f"  ì—í”¼ì†Œë“œ ì™„ë£Œ! ìŠ¤í…: {step + 1}")
                break
        
        episode_rewards.append(episode_reward)
        print(f"  ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.2f}")
        print(f"  ì„±ê³µë¥ : {info['success_rate']:.2f}")
        print(f"  ì¶©ëŒ íšŸìˆ˜: {info['collision_count']}")
        
        # ì—í”¼ì†Œë“œ ê°„ ì ì‹œ ëŒ€ê¸°
        print("  ë‹¤ìŒ ì—í”¼ì†Œë“œê¹Œì§€ ì ì‹œ ëŒ€ê¸°...")
        time.sleep(2.0)
    
    avg_reward = np.mean(episode_rewards)
    print(f"\n=== í‰ê°€ ê²°ê³¼ ===")
    print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {avg_reward:.3f}")
    
    return episode_rewards


def run_informarl_experiment(num_episodes: int = 100, num_agents: int = 6, config: dict = None, gpu_id: int = None, force_cpu: bool = False) -> tuple[List[float], BottleneckInforMARLEnv]:
    """InforMARL ì‹¤í—˜ ì‹¤í–‰"""
    print("=== InforMARL 2D ë³‘ëª© í™˜ê²½ í•™ìŠµ ì‹œì‘ ===")
    
    # ğŸ”¥ ì„¤ì • ë° GPU ì˜µì…˜ì„ í™˜ê²½ì— ì „ë‹¬
    env = BottleneckInforMARLEnv(num_agents=num_agents, config=config, gpu_id=gpu_id, force_cpu=force_cpu)
    episode_rewards = train_agents(env, num_episodes=num_episodes)
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
    print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {np.mean(episode_rewards):.3f}")
    
    return episode_rewards, env


def run_animation_demo(num_agents: int = 4) -> List[float]:
    """ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ì‹¤í–‰"""
    print("=== InforMARL ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents)
    
    # ê°„ë‹¨í•œ í•™ìŠµ (ì„ íƒì‚¬í•­)
    print("ê°„ë‹¨í•œ ì‚¬ì „ í•™ìŠµ ì¤‘...")
    train_agents(env, num_episodes=10)
    
    # ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ í‰ê°€
    print("\ní‰ê°€ ëª¨ë“œ ì• ë‹ˆë©”ì´ì…˜ ì‹œì‘!")
    results = evaluate_with_animation(env, num_episodes=3, render_delay=0.2)
    
    return results


def quick_movement_test(num_agents: int = 2) -> bool:
    """ì—ì´ì „íŠ¸ ì›€ì§ì„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print("=== ì—ì´ì „íŠ¸ ì›€ì§ì„ í…ŒìŠ¤íŠ¸ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents, max_timesteps=50)
    observations = env.reset()
    
    print("ì´ˆê¸° ìƒíƒœ ë Œë”ë§...")
    env.render()
    time.sleep(1)
    
    for step in range(20):
        # ëœë¤ í–‰ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        random_actions = [np.random.randint(0, 4) for _ in range(num_agents)]
        
        print(f"ìŠ¤í… {step}: í–‰ë™ {random_actions}")
        observations, rewards, done, info = env.step(random_actions)
        
        # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ì¶œë ¥
        for i, agent in enumerate(env.agents):
            print(f"  Agent {i}: ({agent.x:.2f}, {agent.y:.2f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
        
        env.render()
        time.sleep(0.5)
        
        if done:
            break
    
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    import matplotlib.pyplot as plt
    plt.show()
    return True