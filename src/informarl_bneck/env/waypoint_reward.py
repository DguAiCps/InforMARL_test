"""
Waypoint-based reward calculation for bottleneck navigation
"""
import numpy as np
import math
import torch
from typing import List
from ..utils.types import Agent2D, Landmark2D
from .path_planner import get_waypoint_distance, get_waypoint_direction


def calculate_waypoint_rewards(agents: List[Agent2D], landmarks: List[Landmark2D]) -> List[float]:
    """Waypoint ê¸°ë°˜ ë³´ìƒ ê³„ì‚° - í›¨ì”¬ í•™ìŠµí•˜ê¸° ì‰¬ìš´ êµ¬ì¡°"""
    rewards = []
    
    for agent in agents:
        reward = 0.0
        target = landmarks[agent.target_id]
        final_distance = agent.get_distance_to(target.x, target.y)
        
        # 1. ìµœì¢… ëª©í‘œ ë„ë‹¬ - ë§¤ìš° í° ë³´ìƒ
        if final_distance < target.radius:
            reward += 200.0
            rewards.append(reward)
            continue
        
        # 2. ê¸°ë³¸ ìƒì¡´ ë³´ìƒ (ë§¤ ìŠ¤í…ë§ˆë‹¤ ì–‘ìˆ˜)
        reward += 0.5
        
        # 3. Waypoint ì§„í–‰ ë³´ìƒ (í•µì‹¬!)
        if hasattr(agent, 'current_waypoint'):
            waypoint_distance = get_waypoint_distance(agent)
            waypoint_direction = get_waypoint_direction(agent)
            
            # 3-1. Waypointì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ë³´ìƒ
            max_distance = 20.0  # ë§µ í¬ê¸° ê¸°ì¤€
            proximity_reward = max(0, (max_distance - waypoint_distance) / max_distance) * 2.0
            reward += proximity_reward
            
            # 3-2. Waypoint ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ë©´ ì¶”ê°€ ë³´ìƒ
            agent_velocity = np.array([agent.vx, agent.vy])
            velocity_magnitude = np.linalg.norm(agent_velocity)
            
            if velocity_magnitude > 0.05:  # ì›€ì§ì´ê³  ìˆì„ ë•Œ
                agent_direction = agent_velocity / velocity_magnitude
                waypoint_dir_vec = np.array(waypoint_direction)
                
                # ë°©í–¥ ì¼ì¹˜ë„ (-1 ~ 1)
                direction_alignment = np.dot(agent_direction, waypoint_dir_vec)
                if direction_alignment > 0:  # ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¼ ë•Œë§Œ
                    reward += direction_alignment * velocity_magnitude * 3.0
            
            # 3-3. Waypoint ë„ë‹¬ ë³´ë„ˆìŠ¤
            if waypoint_distance < 1.0:  # waypointì— ì¶©ë¶„íˆ ê°€ê¹Œìš°ë©´
                reward += 10.0
        
        # 4. ì¶©ëŒ í˜ë„í‹° (ì–‘ë³´ í•™ìŠµì„ ìœ„í•´ ì ì ˆí•œ ê°•ë„)
        collision_penalty = getattr(agent, 'collision_penalty_timer', 0)
        if collision_penalty > 0:
            reward -= 35.0  # ì–‘ë³´ í–‰ë™ í•™ìŠµì„ ìœ„í•œ ì ì ˆí•œ í˜ë„í‹°
        
        # 5. ì •ì§€ í˜ë„í‹° (ì‘ê²Œ)
        agent_velocity = np.array([agent.vx, agent.vy])
        if np.linalg.norm(agent_velocity) < 0.05:
            reward -= 1.0  # ì‘ì€ í˜ë„í‹°
        
        # 6. ìµœì¢… ëª©í‘œ ë°©í–¥ ë³´ë„ˆìŠ¤ (waypointì™€ ë³„ë„)
        target_direction = np.array([target.x - agent.x, target.y - agent.y])
        target_distance_norm = np.linalg.norm(target_direction)
        
        if target_distance_norm > 0.1:
            target_direction = target_direction / target_distance_norm
            agent_velocity = np.array([agent.vx, agent.vy])
            velocity_magnitude = np.linalg.norm(agent_velocity)
            
            if velocity_magnitude > 0.05:
                agent_direction = agent_velocity / velocity_magnitude
                final_alignment = np.dot(agent_direction, target_direction)
                if final_alignment > 0:
                    reward += final_alignment * 0.5  # ì‘ì€ ì¶”ê°€ ë³´ë„ˆìŠ¤
        
        rewards.append(reward)
    
    return rewards


def calculate_waypoint_rewards_gpu(agents: List[Agent2D], landmarks: List[Landmark2D], 
                                 device: torch.device) -> List[float]:
    """ğŸš€ GPU ë³‘ë ¬ ì›¨ì´í¬ì¸íŠ¸ ë³´ìƒ ê³„ì‚° - ëŒ€í­ ì„±ëŠ¥ í–¥ìƒ"""
    num_agents = len(agents)
    if num_agents == 0:
        return []
    
    # ëª¨ë“  ì—ì´ì „íŠ¸ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë°°ì¹˜ ë³€í™˜
    agent_positions = torch.tensor([[agent.x, agent.y] for agent in agents], 
                                  dtype=torch.float32, device=device)  # [N, 2]
    agent_velocities = torch.tensor([[agent.vx, agent.vy] for agent in agents], 
                                   dtype=torch.float32, device=device)  # [N, 2]
    
    # íƒ€ê²Ÿ ì •ë³´ ë°°ì¹˜ ë³€í™˜
    target_positions = torch.tensor([[landmarks[agent.target_id].x, landmarks[agent.target_id].y] 
                                   for agent in agents], dtype=torch.float32, device=device)  # [N, 2]
    target_radii = torch.tensor([landmarks[agent.target_id].radius for agent in agents], 
                               dtype=torch.float32, device=device)  # [N]
    
    # ì›¨ì´í¬ì¸íŠ¸ ì •ë³´ ë°°ì¹˜ ë³€í™˜
    waypoint_positions = torch.tensor([getattr(agent, 'current_waypoint', (agent.x, agent.y)) 
                                     for agent in agents], dtype=torch.float32, device=device)  # [N, 2]
    
    # ì¶©ëŒ í˜ë„í‹° ì •ë³´
    collision_penalties = torch.tensor([getattr(agent, 'collision_penalty_timer', 0) 
                                      for agent in agents], dtype=torch.float32, device=device)  # [N]
    
    # ğŸš€ GPU ë°°ì¹˜ ê³„ì‚° ì‹œì‘
    rewards = torch.zeros(num_agents, dtype=torch.float32, device=device)
    
    # 1. ìµœì¢… ëª©í‘œê¹Œì§€ ê±°ë¦¬ ê³„ì‚° (ë°°ì¹˜)
    final_distances = torch.norm(agent_positions - target_positions, dim=1)  # [N]
    goal_reached = final_distances < target_radii  # [N] boolean
    
    # ëª©í‘œ ë„ë‹¬ ì‹œ 200ì  ë¶€ì—¬í•˜ê³  early return
    rewards[goal_reached] = 200.0
    active_mask = ~goal_reached  # ëª©í‘œì— ë„ë‹¬í•˜ì§€ ì•Šì€ ì—ì´ì „íŠ¸ë“¤
    
    if active_mask.sum() == 0:  # ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ëª©í‘œ ë„ë‹¬
        return rewards.cpu().tolist()
    
    # 2. ê¸°ë³¸ ìƒì¡´ ë³´ìƒ (+0.5)
    rewards[active_mask] += 0.5
    
    # 3. ì›¨ì´í¬ì¸íŠ¸ ì§„í–‰ ë³´ìƒë“¤ (active ì—ì´ì „íŠ¸ë“¤ë§Œ)
    active_agent_pos = agent_positions[active_mask]  # [M, 2]
    active_waypoint_pos = waypoint_positions[active_mask]  # [M, 2]
    active_velocities = agent_velocities[active_mask]  # [M, 2]
    active_target_pos = target_positions[active_mask]  # [M, 2]
    
    # 3-1. ì›¨ì´í¬ì¸íŠ¸ ê·¼ì ‘ ë³´ìƒ
    waypoint_distances = torch.norm(active_agent_pos - active_waypoint_pos, dim=1)  # [M]
    max_distance = 20.0
    proximity_rewards = torch.clamp((max_distance - waypoint_distances) / max_distance, min=0) * 2.0
    rewards[active_mask] += proximity_rewards
    
    # 3-2. ì›¨ì´í¬ì¸íŠ¸ ë°©í–¥ ì´ë™ ë³´ìƒ
    velocity_magnitudes = torch.norm(active_velocities, dim=1)  # [M]
    moving_mask = velocity_magnitudes > 0.05  # ì›€ì§ì´ê³  ìˆëŠ” ì—ì´ì „íŠ¸ë“¤
    
    if moving_mask.sum() > 0:
        # ë°©í–¥ ë²¡í„°ë“¤
        agent_directions = active_velocities[moving_mask] / velocity_magnitudes[moving_mask].unsqueeze(1)  # [K, 2]
        waypoint_directions = active_waypoint_pos[moving_mask] - active_agent_pos[moving_mask]  # [K, 2]
        waypoint_dir_norms = torch.norm(waypoint_directions, dim=1)  # [K]
        
        valid_waypoint_mask = waypoint_dir_norms > 0.01
        if valid_waypoint_mask.sum() > 0:
            waypoint_directions[valid_waypoint_mask] = (waypoint_directions[valid_waypoint_mask] / 
                                                       waypoint_dir_norms[valid_waypoint_mask].unsqueeze(1))
            
            # ë°©í–¥ ì¼ì¹˜ë„ ê³„ì‚° (ë‚´ì )
            direction_alignments = torch.sum(agent_directions * waypoint_directions, dim=1)  # [K]
            positive_alignment = torch.clamp(direction_alignments, min=0)  # ì–‘ìˆ˜ë§Œ
            
            # ë³´ìƒ ê³„ì‚°
            direction_rewards = positive_alignment * velocity_magnitudes[moving_mask] * 3.0
            
            # active_mask ë‚´ì—ì„œ moving_maskì¸ ì¸ë±ìŠ¤ë“¤ì— ë³´ìƒ ì¶”ê°€
            active_indices = torch.where(active_mask)[0]
            moving_indices = active_indices[moving_mask]
            rewards[moving_indices] += direction_rewards
    
    # 3-3. ì›¨ì´í¬ì¸íŠ¸ ë„ë‹¬ ë³´ë„ˆìŠ¤
    waypoint_close = waypoint_distances < 1.0
    rewards[torch.where(active_mask)[0][waypoint_close]] += 10.0
    
    # 4. ì¶©ëŒ í˜ë„í‹° (-35ì )
    collision_mask = collision_penalties > 0
    rewards[collision_mask] -= 35.0
    
    # 5. ì •ì§€ í˜ë„í‹° (-1ì )
    all_velocity_magnitudes = torch.norm(agent_velocities, dim=1)
    stationary_mask = all_velocity_magnitudes < 0.05
    rewards[stationary_mask] -= 1.0
    
    # 6. ìµœì¢… ëª©í‘œ ë°©í–¥ ë³´ë„ˆìŠ¤
    target_directions = target_positions - agent_positions  # [N, 2]
    target_distances = torch.norm(target_directions, dim=1)  # [N]
    
    valid_target_mask = target_distances > 0.1
    if valid_target_mask.sum() > 0:
        target_directions[valid_target_mask] = (target_directions[valid_target_mask] / 
                                               target_distances[valid_target_mask].unsqueeze(1))
        
        moving_all_mask = all_velocity_magnitudes > 0.05
        final_bonus_mask = valid_target_mask & moving_all_mask
        
        if final_bonus_mask.sum() > 0:
            agent_dirs_all = agent_velocities[final_bonus_mask] / all_velocity_magnitudes[final_bonus_mask].unsqueeze(1)
            final_alignments = torch.sum(agent_dirs_all * target_directions[final_bonus_mask], dim=1)
            positive_final_alignments = torch.clamp(final_alignments, min=0)
            rewards[final_bonus_mask] += positive_final_alignments * 0.5
    
    return rewards.cpu().tolist()