"""
Main bottleneck environment for InforMARL
"""
import gymnasium as gym
from gymnasium import spaces
import torch
from torch_geometric.data import Data, Batch
from typing import List, Dict, Any
import numpy as np

from ..utils.types import Agent2D, Landmark2D, Obstacle2D
from ..models import GraphNeuralNetwork, InforMARLAgent
from ..utils.device import get_device, setup_gpu_environment
from .map import create_agents_and_landmarks, create_obstacles
from .physics import execute_action, update_positions, batch_execute_actions_gpu, batch_update_positions_gpu
from .reward import calculate_rewards
from .waypoint_reward import calculate_waypoint_rewards, calculate_waypoint_rewards_gpu
from .graph_builder import build_graph_observations, batch_build_graph_observations_gpu
from .render import BottleneckRenderer
from .path_planner import update_agent_waypoints, get_waypoint_direction, get_waypoint_distance


class BottleneckInforMARLEnv(gym.Env):
    """2D ë³‘ëª© í™˜ê²½ - InforMARL ê¸°ë°˜"""
    
    def __init__(self, 
                 num_agents: int = 6,
                 agent_radius: float = 0.5,
                 corridor_width: float = 20.0,
                 corridor_height: float = 10.0,
                 bottleneck_width: float = 1.2,
                 bottleneck_position: float = 10.0,
                 sensing_radius: float = 3.0,
                 max_timesteps: int = 300,
                 config: dict = None,  # ğŸ”¥ YAML ì„¤ì •ì„ ë°›ì„ ìˆ˜ ìˆê²Œ
                 train_config: dict = None,  # ğŸš€ train.yaml ì„¤ì •
                 gpu_id: int = None,   # ğŸš€ ì„œë²„ GPU ID ì§€ì •
                 force_cpu: bool = False):  # CPU ê°•ì œ ì‚¬ìš©
        
        super().__init__()
        
        # ğŸš€ GPU í™˜ê²½ ì„¤ì •
        setup_gpu_environment()
        self.device = get_device(gpu_id=gpu_id, force_cpu=force_cpu)
        
        # ì„¤ì • ì €ì¥
        self.config = config
        self.train_config = train_config or {}
        
        # ğŸ”¥ YAML ì„¤ì •ì´ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
        if config is not None:
            self.num_agents = config.get('num_agents', num_agents)
            self.agent_radius = config.get('agent_radius', agent_radius)
            self.corridor_width = config.get('corridor_width', corridor_width)
            self.corridor_height = config.get('corridor_height', corridor_height)
            self.bottleneck_width = config.get('bottleneck_width', bottleneck_width)
            self.bottleneck_position = config.get('bottleneck_position', bottleneck_position)
            self.sensing_radius = config.get('sensing_radius', sensing_radius)
            self.max_timesteps = config.get('max_timesteps', max_timesteps)
            
            # ì„±ëŠ¥ ì„¤ì •
            self.use_gpu_graph = config.get('use_gpu_graph', False)
            self.include_obstacles_in_gnn = config.get('include_obstacles_in_gnn', True)
            force_cpu = config.get('force_cpu', force_cpu)
        else:
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.num_agents = num_agents
            self.agent_radius = agent_radius
            self.corridor_width = corridor_width
            self.corridor_height = corridor_height
            self.bottleneck_width = bottleneck_width
            self.bottleneck_position = bottleneck_position
            self.sensing_radius = sensing_radius
            self.max_timesteps = max_timesteps
            self.use_gpu_graph = False  # ê¸°ë³¸ê°’: CPU ê·¸ë˜í”„
            self.include_obstacles_in_gnn = True  # ê¸°ë³¸ê°’: ì¥ì• ë¬¼ í¬í•¨
        
        # í–‰ë™ ê³µê°„: [ìœ„, ì•„ë˜, ì™¼ìª½, ì˜¤ë¥¸ìª½]
        self.action_space = spaces.Discrete(4)
        
        # í™˜ê²½ ìƒíƒœ
        self.agents: List[Agent2D] = []
        self.landmarks: List[Landmark2D] = []
        self.obstacles: List[Obstacle2D] = []
        self.informarl_agents: List[InforMARLAgent] = []
        
        # ê³µìœ  GNNê³¼ ì˜µí‹°ë§ˆì´ì €
        self.shared_gnn = None
        self.gnn_optimizer = None
        
        # ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ
        self.timestep = 0
        self.success_count = 0
        self.collision_count = 0
        
        # ë Œë”ë§
        self.renderer = BottleneckRenderer()
        
        # ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self._initialize_shared_networks()
        
    def _initialize_shared_networks(self):
        """ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” (í•­ìƒ ìˆ˜í–‰)"""
        # GNN ì´ˆê¸°í™”
        if self.shared_gnn is None:
            gnn_config = {}
            if hasattr(self, 'config') and self.config and 'model' in self.config and 'gnn' in self.config['model']:
                gnn_config = self.config['model']['gnn']
            
            self.shared_gnn = GraphNeuralNetwork(
                input_dim=gnn_config.get('input_dim', 6),
                hidden_dim=gnn_config.get('hidden_dim', 64),
                num_layers=gnn_config.get('num_layers', 1),
                num_embeddings=gnn_config.get('num_embeddings', 4),
                embedding_size=gnn_config.get('embedding_size', 8),
                edge_dim=gnn_config.get('edge_dim', 1),
                use_attention=gnn_config.get('use_attention', True)
            ).to(self.device)
            
            learning_rate = 0.003
            if hasattr(self, 'train_config') and self.train_config and 'training' in self.train_config:
                learning_rate = self.train_config['training'].get('learning_rate', 0.003)
            
            self.gnn_optimizer = torch.optim.Adam(self.shared_gnn.parameters(), lr=learning_rate)
        
        # Actor/Critic ì´ˆê¸°í™”
        if not hasattr(self, 'shared_actor') or self.shared_actor is None:
            from ..models.policy import Actor, Critic
            
            actor_config = {}
            critic_config = {}
            if self.config and 'model' in self.config:
                model_cfg = self.config['model']
                actor_config = model_cfg.get('actor', {})
                critic_config = model_cfg.get('critic', {})
            
            self.shared_actor = Actor(
                obs_dim=actor_config.get('obs_dim', 9),
                agg_dim=actor_config.get('agg_dim', 64),
                action_dim=actor_config.get('action_dim', 4),
                hidden_dim=actor_config.get('hidden_dim', 64)
            ).to(self.device)
            
            self.shared_critic = Critic(
                agg_dim=critic_config.get('agg_dim', 64),
                hidden_dim=critic_config.get('hidden_dim', 64)
            ).to(self.device)
            
            learning_rate = 0.003
            if hasattr(self, 'train_config') and self.train_config and 'training' in self.train_config:
                learning_rate = self.train_config['training'].get('learning_rate', 0.003)
            
            self.shared_policy_optimizer = torch.optim.Adam(
                list(self.shared_actor.parameters()) + list(self.shared_critic.parameters()), 
                lr=learning_rate
            )

    def reset(self) -> List[Data]:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.timestep = 0
        self.success_count = 0
        self.collision_count = 0
        
        # ë²½ ì—¬ë°± ê³„ì‚°
        obstacle_spacing = (self.agent_radius * 2) / 3
        obstacle_radius = obstacle_spacing / 2
        wall_margin = obstacle_radius * 3
        
        # í™˜ê²½ ê°ì²´ ìƒì„±
        self.agents, self.landmarks = create_agents_and_landmarks(
            self.num_agents, self.corridor_width, self.corridor_height,
            self.agent_radius, wall_margin
        )
        
        self.obstacles = create_obstacles(
            self.corridor_width, self.corridor_height,
            self.bottleneck_position, self.bottleneck_width,
            self.agent_radius
        )
        
        # InforMARL ì—ì´ì „íŠ¸ ìƒì„± (GPU ì‚¬ìš©)
        self.informarl_agents = []
        for i in range(self.num_agents):
            informarl_agent = InforMARLAgent(agent_id=i, device=self.device)
            self.informarl_agents.append(informarl_agent)
        
        # ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™” ìˆ˜í–‰
        self._initialize_shared_networks()
        
        # ì—ì´ì „íŠ¸ waypoint ì´ˆê¸°í™”
        update_agent_waypoints(
            self.agents, self.landmarks, self.corridor_width, self.corridor_height,
            self.bottleneck_position, self.bottleneck_width
        )
        
        # ê·¸ë˜í”„ ìƒì„± (ì„¤ì •ì— ë”°ë¼ GPU/CPU ì„ íƒ)
        if self.use_gpu_graph:
            try:
                return batch_build_graph_observations_gpu(
                    self.agents, self.landmarks, self.obstacles, self.sensing_radius, 
                    self.device, self.include_obstacles_in_gnn
                )
            except Exception as e:
                print(f"GPU graph building failed in reset, using CPU: {e}")
                return build_graph_observations(
                    self.agents, self.landmarks, self.obstacles, self.sensing_radius,
                    self.include_obstacles_in_gnn
                )
        else:
            return build_graph_observations(
                self.agents, self.landmarks, self.obstacles, self.sensing_radius,
                self.include_obstacles_in_gnn
            )
    
    def step(self, actions: List[int] = None):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.timestep += 1
        
        # ê·¸ë˜í”„ ê´€ì¸¡ ìƒì„± (ì„¤ì •ì— ë”°ë¼ GPU/CPU ì„ íƒ)
        if self.use_gpu_graph:
            try:
                graph_obs = batch_build_graph_observations_gpu(
                    self.agents, self.landmarks, self.obstacles, self.sensing_radius, 
                    self.device, self.include_obstacles_in_gnn
                )
            except Exception as e:
                print(f"GPU graph building failed, using CPU: {e}")
                graph_obs = build_graph_observations(
                    self.agents, self.landmarks, self.obstacles, self.sensing_radius,
                    self.include_obstacles_in_gnn
                )
        else:
            graph_obs = build_graph_observations(
                self.agents, self.landmarks, self.obstacles, self.sensing_radius,
                self.include_obstacles_in_gnn
            )
        
        # waypoint ì—…ë°ì´íŠ¸ (5ìŠ¤í…ë§ˆë‹¤ë¡œ ì¤„ì„ - ì„±ëŠ¥ ìµœì í™”)
        if self.timestep % 5 == 0:
            update_agent_waypoints(
                self.agents, self.landmarks, self.corridor_width, self.corridor_height,
                self.bottleneck_position, self.bottleneck_width
            )
        
        # í–‰ë™ ì„ íƒ (ë°°ì¹˜ ì²˜ë¦¬)
        if actions is None:
            actions, log_probs, values = self._get_batch_actions(graph_obs, training=True)
        else:
            log_probs = [0.0] * len(actions)
            values = [0.0] * len(actions)
        
        # ğŸš€ GPU ë°°ì¹˜ ë¬¼ë¦¬ ê³„ì‚° (ê¸°ì¡´ CPU ë°©ì‹ë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„)
        try:
            # GPUì—ì„œ í–‰ë™ ì‹¤í–‰ (ë°°ì¹˜)
            new_velocities = batch_execute_actions_gpu(self.agents, actions, self.device)
            
            # GPUì—ì„œ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (ë°°ì¹˜)
            collision_count = batch_update_positions_gpu(
                self.agents, new_velocities, self.obstacles, 
                self.corridor_width, self.corridor_height,
                self.bottleneck_position, self.bottleneck_width, self.device
            )
        except Exception as e:
            # GPU ì‹¤íŒ¨ ì‹œ CPU ë°±ì—…
            print(f"GPU physics failed, using CPU: {e}")
            for i, action in enumerate(actions):
                execute_action(self.agents[i], action)
            
            collision_count = update_positions(
                self.agents, self.obstacles, self.corridor_width, self.corridor_height,
                self.bottleneck_position, self.bottleneck_width
            )
        self.collision_count += collision_count
        
        # ë³´ìƒ ê³„ì‚° (GPU ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”)
        try:
            rewards = calculate_waypoint_rewards_gpu(self.agents, self.landmarks, self.device)
        except Exception as e:
            print(f"GPU reward calculation failed, using CPU: {e}")
            rewards = calculate_waypoint_rewards(self.agents, self.landmarks)
        
        # ì„±ê³µ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        for agent in self.agents:
            target = self.landmarks[agent.target_id]
            if agent.get_distance_to(target.x, target.y) < target.radius:
                self.success_count += 1
        
        # ê²½í—˜ ì €ì¥
        if actions is not None:
            for i, (action, log_prob, value, reward) in enumerate(zip(actions, log_probs, values, rewards)):
                obs = self._get_local_observation(i)
                experience = {
                    'graph_data': graph_obs[i],
                    'local_obs': torch.tensor(obs, dtype=torch.float32),
                    'action': action,
                    'log_prob': log_prob,
                    'value': value,
                    'reward': reward
                }
                self.informarl_agents[i].store_experience(experience)
        
        # ğŸš€ ì„±ëŠ¥ ìµœì í™”: step ë ê·¸ë˜í”„ ìƒì„± ì œê±° (ë‹¤ìŒ ìŠ¤í…ì—ì„œ ì–´ì°¨í”¼ ìƒˆë¡œ ìƒì„±)
        done = self._is_done()
        info = self._get_info()
        
        # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        return [], rewards, done, info
    
    def _get_batch_actions(self, graph_observations: List[Data], training: bool = True):
        """ê°œë³„ ê·¸ë˜í”„ ì²˜ë¦¬ - ì„¼ì‹± ë²”ìœ„ ì œí•œ ìœ ì§€í•˜ë©´ì„œ GPU ì‚¬ìš©"""
        device = self.device
        
        actions = []
        log_probs = []
        values = []
        
        # ğŸš€ ë¡œì»¬ ê´€ì¸¡ì„ ë°°ì¹˜ë¡œ GPU ì „ì†¡ (ì—¬ì „íˆ ìµœì í™” ê°€ëŠ¥)
        all_local_obs = []
        for i in range(self.num_agents):
            obs = self._get_local_observation(i)
            all_local_obs.append(obs)
        local_obs_batch = torch.tensor(all_local_obs, dtype=torch.float32).to(device, non_blocking=True)
        
        # ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëª¨ë“  ê·¸ë˜í”„ë¥¼ í•œ ë²ˆì— GNN í†µê³¼
        batch_graphs = Batch.from_data_list(graph_observations).to(device, non_blocking=True)
        all_embeddings = self.shared_gnn(batch_graphs)
        
        # ê° ê·¸ë˜í”„ë³„ë¡œ ì„ë² ë”© ë¶„ë¦¬
        agent_embeddings = []
        global_embeddings = []
        
        current_idx = 0
        for i in range(self.num_agents):
            graph_size = len(graph_observations[i].x)
            
            # ì´ ê·¸ë˜í”„ì˜ ì„ë² ë”© ì¶”ì¶œ
            graph_embeddings = all_embeddings[current_idx:current_idx + graph_size]
            
            # ì—ì´ì „íŠ¸ ìì‹ ì˜ ì„ë² ë”© (ë³´í†µ ì²« ë²ˆì§¸ ë…¸ë“œ)
            ego_embedding = graph_embeddings[i] if i < len(graph_embeddings) else graph_embeddings[0]
            agent_embeddings.append(ego_embedding)
            
            # ì „ì—­ ì§‘ê³„ë¥¼ ìœ„í•œ ëª¨ë“  ì—ì´ì „íŠ¸ ë…¸ë“œë“¤ì˜ í‰ê· 
            # ì„¼ì‹± ë²”ìœ„ ë‚´ ì—ì´ì „íŠ¸ë“¤ë§Œ í¬í•¨ (ë…¼ë¬¸ì˜ í•µì‹¬!)
            agent_indices = []
            for j, entity_type in enumerate(graph_observations[i].entity_type):
                if entity_type == 0:  # agent íƒ€ì…
                    agent_indices.append(j)
            
            if agent_indices:
                agent_nodes = graph_embeddings[agent_indices]
                global_agg = agent_nodes.mean(dim=0)
            else:
                global_agg = ego_embedding
            
            global_embeddings.append(global_agg)
            
            current_idx += graph_size
        
        # GPUì—ì„œ ë°°ì¹˜ ì²˜ë¦¬
        agent_embeddings_batch = torch.stack(agent_embeddings)
        global_embeddings_batch = torch.stack(global_embeddings)
        
        # ğŸš€ ì§„ì§œ ë°°ì¹˜ ì²˜ë¦¬ - ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ í•œë²ˆì— ì²˜ë¦¬
        if training:
            # ğŸš€ ê³µìœ  Criticìœ¼ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ê°’ í•¨ìˆ˜ë¥¼ í•œë²ˆì— ê³„ì‚°
            global_values = self.shared_critic(global_embeddings_batch)  # [N, 1]
            
            # ğŸš€ ê³µìœ  Actorë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ì˜ í–‰ë™ í™•ë¥ ì„ í•œë²ˆì— ê³„ì‚°
            all_action_probs = self.shared_actor(local_obs_batch, agent_embeddings_batch)  # [N, 4]
            
            # ğŸš€ ë°°ì¹˜ë¡œ í–‰ë™ ì„ íƒ
            action_dists = torch.distributions.Categorical(all_action_probs)
            sampled_actions = action_dists.sample()  # [N]
            log_probs_tensor = action_dists.log_prob(sampled_actions)  # [N]
            
            actions = sampled_actions.cpu().tolist()
            log_probs = log_probs_tensor.cpu().tolist()
            values = global_values.squeeze().cpu().tolist()
        else:
            # ğŸš€ í‰ê°€ ì‹œë„ ë°°ì¹˜ ì²˜ë¦¬
            all_action_probs = self.shared_actor(local_obs_batch, agent_embeddings_batch)  # [N, 4]
            
            # ê²°ì •ì  í–‰ë™ ì„ íƒ (ê°€ì¥ ë†’ì€ í™•ë¥ )
            sampled_actions = torch.argmax(all_action_probs, dim=1)  # [N]
            actions = sampled_actions.cpu().tolist()
            log_probs = [0.0] * len(actions)
            values = [0.0] * len(actions)
        
        return actions, log_probs, values
    
    def _update_shared_networks(self):
        """ğŸš€ ê³µìœ  ë„¤íŠ¸ì›Œí¬ ë°°ì¹˜ ì—…ë°ì´íŠ¸ - ëª¨ë“  ì—ì´ì „íŠ¸ ê²½í—˜ì„ í•œë²ˆì— ì²˜ë¦¬"""
        # ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ê²½í—˜ì„ ëª¨ìŒ
        all_experiences = []
        for agent in self.informarl_agents:
            if len(agent.memory) >= 16:  # ìµœì†Œ ê²½í—˜ì´ ìˆëŠ” ì—ì´ì „íŠ¸ë§Œ
                experiences = list(agent.memory)
                all_experiences.extend(experiences[-32:])  # ìµœê·¼ 32ê°œë§Œ ì‚¬ìš©
        
        if len(all_experiences) < 32:  # ì „ì²´ ê²½í—˜ì´ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            return
        
        # ğŸš€ ë°°ì¹˜ ìƒ˜í”Œë§ (train.yaml ì„¤ì • ì‚¬ìš©)
        import random
        
        # train.yamlì—ì„œ batch_size ì‚¬ìš©
        default_batch_size = 64
        if hasattr(self, 'train_config') and 'training' in self.train_config:
            default_batch_size = self.train_config['training'].get('batch_size', 64)
        
        batch_size = min(default_batch_size, len(all_experiences))
        batch = random.sample(all_experiences, batch_size)
        
        print(f"  ë°°ì¹˜ í•™ìŠµ: {batch_size}ê°œ ê²½í—˜ ì‚¬ìš© (ì „ì²´ {len(all_experiences)}ê°œ ì¤‘)")
        
        # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        device = self.device
        graph_data_list = [exp['graph_data'] for exp in batch]
        local_obs = torch.stack([exp['local_obs'] for exp in batch]).to(device)
        actions = torch.tensor([exp['action'] for exp in batch]).to(device)
        old_log_probs = torch.tensor([exp['log_prob'] for exp in batch], dtype=torch.float32).to(device)
        rewards = torch.tensor([exp['reward'] for exp in batch], dtype=torch.float32).to(device)
        values = torch.tensor([exp['value'] for exp in batch], dtype=torch.float32).to(device)
        
        # GAE ê³„ì‚°
        advantages = self._compute_gae(rewards, values).to(device)
        returns = (advantages + values).to(device)
        
        # ğŸš€ PPO ì—…ë°ì´íŠ¸ (train.yamlì—ì„œ ppo_epochs ì‚¬ìš©)
        default_ppo_epochs = 3
        if hasattr(self, 'train_config') and 'training' in self.train_config:
            default_ppo_epochs = self.train_config['training'].get('ppo_epochs', 3)
        
        for epoch in range(default_ppo_epochs):
            # ê³µìœ  GNNìœ¼ë¡œ ê·¸ë˜í”„ ë°ì´í„° ì²˜ë¦¬
            from torch_geometric.data import Batch
            batch_graphs = Batch.from_data_list(graph_data_list).to(device)
            node_embeddings = self.shared_gnn(batch_graphs)
            
            # ì—ì´ì „íŠ¸ ì„ë² ë”© ì¶”ì¶œ (simplified)
            nodes_per_graph = len(graph_data_list[0].x) if graph_data_list else 1
            agent_embeddings = []
            global_embeddings = []
            
            for i in range(len(batch)):
                start_idx = i * nodes_per_graph
                end_idx = min(start_idx + nodes_per_graph, len(node_embeddings))
                
                if start_idx < len(node_embeddings):
                    agent_emb = node_embeddings[start_idx]
                    agent_embeddings.append(agent_emb)
                    
                    # ê¸€ë¡œë²Œ ì§‘ê³„ (ë‹¨ìˆœí™”)
                    graph_nodes = node_embeddings[start_idx:end_idx]
                    global_agg = graph_nodes.mean(dim=0) if len(graph_nodes) > 0 else agent_emb
                    global_embeddings.append(global_agg)
                else:
                    # í´ë°±
                    agent_embeddings.append(node_embeddings[0] if len(node_embeddings) > 0 else torch.zeros(64, device=device))
                    global_embeddings.append(node_embeddings[0] if len(node_embeddings) > 0 else torch.zeros(64, device=device))
            
            agent_embeddings = torch.stack(agent_embeddings)
            global_embeddings = torch.stack(global_embeddings)
            
            # ğŸš€ ê³µìœ  ë„¤íŠ¸ì›Œí¬ë¡œ ë°°ì¹˜ ì²˜ë¦¬
            action_probs = self.shared_actor(local_obs, agent_embeddings)
            current_values = self.shared_critic(global_embeddings).squeeze()
            
            # PPO ì†ì‹¤ ê³„ì‚°
            dist = torch.distributions.Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy()
            
            # train.yamlì—ì„œ PPO íŒŒë¼ë¯¸í„° ì‚¬ìš©
            clip_eps = 0.2  # ê¸°ë³¸ê°’
            value_coef = 0.5  # ê¸°ë³¸ê°’
            entropy_coef = 0.01  # ê¸°ë³¸ê°’
            
            if hasattr(self, 'train_config') and 'training' in self.train_config:
                training = self.train_config['training']
                clip_eps = training.get('clip_epsilon', 0.2)
                value_coef = training.get('value_loss_coef', 0.5)
                entropy_coef = training.get('entropy_coef', 0.01)
            
            # PPO ratio
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # Policy loss (clipped)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # Value loss
            value_loss = torch.nn.functional.mse_loss(current_values, returns)
            
            # Entropy bonus
            entropy_loss = -entropy.mean()
            
            # Total loss
            total_loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss
            
            # ğŸš€ ê³µìœ  ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (train.yamlì—ì„œ max_grad_norm ì‚¬ìš©)
            max_grad_norm = 0.5  # ê¸°ë³¸ê°’
            if hasattr(self, 'train_config') and 'training' in self.train_config:
                max_grad_norm = self.train_config['training'].get('max_grad_norm', 0.5)
            
            self.shared_policy_optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.shared_actor.parameters()) + list(self.shared_critic.parameters()), 
                max_grad_norm
            )
            self.shared_policy_optimizer.step()
    
    def _compute_gae(self, rewards, values, gamma=None, lam=None):
        """train.yamlì—ì„œ GAE íŒŒë¼ë¯¸í„° ì‚¬ìš©"""
        # train.yamlì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
        if gamma is None:
            gamma = 0.99  # ê¸°ë³¸ê°’
            if hasattr(self, 'train_config') and 'training' in self.train_config:
                gamma = self.train_config['training'].get('gamma', 0.99)
        
        if lam is None:
            lam = 0.95  # ê¸°ë³¸ê°’
            if hasattr(self, 'train_config') and 'training' in self.train_config:
                lam = self.train_config['training'].get('lambda', 0.95)
        """Generalized Advantage Estimation"""
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                delta = rewards[t] - values[t]
            else:
                delta = rewards[t] + gamma * values[t+1] - values[t]
            
            gae = delta + gamma * lam * gae
            advantages[t] = gae
        
        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    
    def _get_local_observation(self, agent_id: int) -> List[float]:
        """ì—ì´ì „íŠ¸ì˜ ë¡œì»¬ ê´€ì¸¡ (waypoint í¬í•¨)"""
        agent = self.agents[agent_id]
        target = self.landmarks[agent.target_id]
        
        # ê¸°ë³¸ ê´€ì¸¡
        obs = [
            agent.x / self.sensing_radius,    # sensing_radiusë¡œ ì •ê·œí™”ëœ ìœ„ì¹˜
            agent.y / self.sensing_radius,
            agent.vx / agent.max_speed,       # ì •ê·œí™”ëœ ì†ë„
            agent.vy / agent.max_speed,
            (target.x - agent.x) / self.sensing_radius,  # ìƒëŒ€ ëª©í‘œ ìœ„ì¹˜
            (target.y - agent.y) / self.sensing_radius
        ]
        
        # waypoint ì •ë³´ ì¶”ê°€
        waypoint_direction = get_waypoint_direction(agent)
        waypoint_distance = get_waypoint_distance(agent)
        
        obs.extend([
            waypoint_direction[0],  # waypoint ë°©í–¥ x
            waypoint_direction[1],  # waypoint ë°©í–¥ y  
            waypoint_distance / self.sensing_radius  # ì •ê·œí™”ëœ waypoint ê±°ë¦¬
        ])
        
        return obs
    
    def _is_done(self) -> bool:
        """ì¢…ë£Œ ì¡°ê±´"""
        if self.timestep >= self.max_timesteps:
            return True
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ëª©í‘œ ë„ë‹¬
        for agent in self.agents:
            target = self.landmarks[agent.target_id]
            if agent.get_distance_to(target.x, target.y) >= target.radius:
                return False
        return True
    
    def _get_info(self) -> Dict[str, Any]:
        """ì •ë³´ ë°˜í™˜"""
        return {
            "timestep": self.timestep,
            "success_count": self.success_count,
            "collision_count": self.collision_count,
            "success_rate": self.success_count / max(1, self.num_agents),
            "avg_time_ratio": self.timestep / self.max_timesteps
        }
    
    def render(self, mode='human', show_waypoints=True):
        """í™˜ê²½ ë Œë”ë§"""
        self.renderer.render(
            self.agents, self.landmarks, self.obstacles,
            self.corridor_width, self.corridor_height,
            self.bottleneck_position, self.bottleneck_width,
            self.timestep, self.success_count, self.collision_count,
            show_waypoints=show_waypoints
        )