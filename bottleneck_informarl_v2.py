"""
2D ë³‘ëª© ì‹œë‚˜ë¦¬ì˜¤ + InforMARL êµ¬í˜„ - ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ë°˜
ì›í˜• ì—ì´ì „íŠ¸, ë³‘ëª© í†µë¡œ, ìƒí•˜ì¢Œìš° ì´ë™
InforMARL ë…¼ë¬¸ ë° ê¸°ì¡´ êµ¬í˜„ì„ ì •í™•íˆ ë”°ë¦„
"""
from dataclasses import dataclass
from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import gymnasium as gym
from gymnasium import spaces
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.animation as animation
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.data import Data, Batch
from torch_geometric.utils import add_self_loops
import random
from collections import deque
import math


class EmbedConv(MessagePassing):
    """ê¸°ì¡´ InforMARL í”„ë¡œì íŠ¸ì˜ EmbedConv ë ˆì´ì–´"""
    
    def __init__(self, 
                 input_dim: int = 6,  # [pos, vel, goal] = 6ì°¨ì›
                 num_embeddings: int = 3,  # agent=0, landmark=1, obstacle=2
                 embedding_size: int = 8,
                 hidden_size: int = 64,
                 layer_N: int = 2,
                 edge_dim: int = 1):  # ê±°ë¦¬ ì •ë³´
        super(EmbedConv, self).__init__(aggr='add')
        
        self.entity_embed = nn.Embedding(num_embeddings, embedding_size)
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´: [node_features + embedding + edge_features] -> hidden
        self.lin1 = nn.Linear(input_dim + embedding_size + edge_dim, hidden_size)
        
        # ì€ë‹‰ ë ˆì´ì–´ë“¤
        self.layers = nn.ModuleList()
        for _ in range(layer_N):
            self.layers.append(nn.Linear(hidden_size, hidden_size))
        
        self.activation = nn.ReLU()
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def forward(self, x, edge_index, edge_attr=None, entity_type=None):
        """
        x: [num_nodes, input_dim] - ë…¸ë“œ íŠ¹ì§•
        edge_index: [2, num_edges] - ì—£ì§€ ì—°ê²°
        edge_attr: [num_edges, edge_dim] - ì—£ì§€ íŠ¹ì§• (ê±°ë¦¬)
        entity_type: [num_nodes] - ì—”í‹°í‹° íƒ€ì…
        """
        # ì—”í‹°í‹° íƒ€ì… ì„ë² ë”©
        if entity_type is not None:
            entity_emb = self.entity_embed(entity_type.long())
            x = torch.cat([x, entity_emb], dim=-1)
        
        # ë©”ì‹œì§€ íŒ¨ì‹±
        return self.propagate(edge_index, x=x, edge_attr=edge_attr)
    
    def message(self, x_j, edge_attr=None):
        """ë©”ì‹œì§€ ìƒì„±"""
        if edge_attr is not None:
            msg = torch.cat([x_j, edge_attr], dim=-1)
        else:
            msg = x_j
            
        # ì²« ë²ˆì§¸ ë ˆì´ì–´
        msg = self.activation(self.lin1(msg))
        msg = self.layer_norm(msg)
        
        # ì€ë‹‰ ë ˆì´ì–´ë“¤
        for layer in self.layers:
            msg = self.activation(layer(msg))
            msg = self.layer_norm(msg)
            
        return msg


class GraphNeuralNetwork(nn.Module):
    """InforMARLìš© GNN - ë…¼ë¬¸ì˜ ì œì•½ ì¡°ê±´ì— ë§ê²Œ ìˆ˜ì •"""
    
    def __init__(self, 
                 input_dim: int = 6,
                 hidden_dim: int = 64, 
                 num_layers: int = 1,  # ê´€ì¸¡ ë²”ìœ„ ì œí•œì„ ìœ„í•´ 1 layerë¡œ ì œí•œ
                 num_embeddings: int = 4,  # agent, landmark, obstacle, wall
                 embedding_size: int = 8,
                 edge_dim: int = 1,
                 use_attention: bool = True):
        super().__init__()
        
        self.num_layers = num_layers
        self.use_attention = use_attention
        
        # ì²« ë²ˆì§¸ (ê·¸ë¦¬ê³  ìœ ì¼í•œ) ë ˆì´ì–´ - ì§ì ‘ ê´€ì¸¡ ì •ë³´ë§Œ ì²˜ë¦¬
        self.embed_conv = EmbedConv(
            input_dim=input_dim,
            num_embeddings=num_embeddings,
            embedding_size=embedding_size,
            hidden_size=hidden_dim,
            layer_N=1,
            edge_dim=edge_dim
        )
        
        # Attention ë©”ì»¤ë‹ˆì¦˜ (ë…¼ë¬¸ì˜ selective prioritization)
        if self.use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_dim,
                num_heads=4,
                dropout=0.1,
                batch_first=True
            )
            self.attention_norm = nn.LayerNorm(hidden_dim)
        
        self.output_layer = nn.Linear(hidden_dim, hidden_dim)
        
    def forward(self, data):
        """
        ë…¼ë¬¸ì˜ ì œì•½ ì¡°ê±´ì— ë§ëŠ” ë‹¨ì¼ layer ì²˜ë¦¬
        - ì§ì ‘ ê´€ì¸¡ ì •ë³´ë§Œ ì‚¬ìš© (1-hopë§Œ)
        - Attentionì„ í†µí•œ selective prioritization
        """
        x = data.x
        edge_index = data.edge_index
        edge_attr = data.edge_attr
        entity_type = data.entity_type
        
        # ë‹¨ì¼ GNN ë ˆì´ì–´ë¡œ ì§ì ‘ ê´€ì¸¡ ì •ë³´ë§Œ ì²˜ë¦¬
        x = self.embed_conv(x, edge_index, edge_attr, entity_type)
        
        # Attention ë©”ì»¤ë‹ˆì¦˜ ì ìš© (ë…¼ë¬¸ì˜ selective prioritization)
        if self.use_attention:
            # xë¥¼ batch í˜•íƒœë¡œ ë³€í™˜ [1, num_nodes, hidden_dim]
            x_batch = x.unsqueeze(0)
            
            # Self-attentionìœ¼ë¡œ ì´ì›ƒ ë…¸ë“œ ì •ë³´ì˜ ì¤‘ìš”ë„ ê³„ì‚°
            attn_output, _ = self.attention(x_batch, x_batch, x_batch)
            
            # Residual connectionê³¼ layer normalization
            x = x + attn_output.squeeze(0)
            x = self.attention_norm(x)
        
        # ìµœì¢… ì¶œë ¥ ë³€í™˜
        x = self.output_layer(x)
        
        return x


class Actor(nn.Module):
    """ì•¡í„° ë„¤íŠ¸ì›Œí¬ - ê°œë³„ ì—ì´ì „íŠ¸ ì„ë² ë”© ì‚¬ìš©"""
    
    def __init__(self, 
                 obs_dim: int = 6,  # ë¡œì»¬ ê´€ì¸¡ ì°¨ì›
                 agg_dim: int = 64,  # ì§‘ê³„ëœ ì •ë³´ ì°¨ì›
                 action_dim: int = 4,
                 hidden_dim: int = 64):
        super().__init__()
        
        # [ë¡œì»¬ ê´€ì¸¡ + ì§‘ê³„ ì •ë³´] ì—°ê²°
        input_dim = obs_dim + agg_dim
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.action_head = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, obs, agg_info):
        """
        obs: [batch, obs_dim] - ë¡œì»¬ ê´€ì¸¡
        agg_info: [batch, agg_dim] - ì§‘ê³„ëœ ì •ë³´
        """
        x = torch.cat([obs, agg_info], dim=-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        action_probs = F.softmax(self.action_head(x), dim=-1)
        return action_probs


class Critic(nn.Module):
    """í¬ë¦¬í‹± ë„¤íŠ¸ì›Œí¬ - ì „ì—­ ì§‘ê³„ ì •ë³´ ì‚¬ìš©"""
    
    def __init__(self, agg_dim: int = 64, hidden_dim: int = 64):
        super().__init__()
        
        self.fc1 = nn.Linear(agg_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.value_head = nn.Linear(hidden_dim, 1)
        
    def forward(self, global_agg):
        """
        global_agg: [batch, agg_dim] - ì „ì—­ ì§‘ê³„ ì •ë³´
        """
        x = F.relu(self.fc1(global_agg))
        x = F.relu(self.fc2(x))
        value = self.value_head(x)
        return value


class InforMARLAgent:
    """InforMARL ì—ì´ì „íŠ¸"""
    
    def __init__(self, agent_id: int, obs_dim: int = 6, action_dim: int = 4):
        self.agent_id = agent_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.gnn = GraphNeuralNetwork().to(self.device)
        self.actor = Actor(obs_dim=obs_dim, action_dim=action_dim).to(self.device)
        self.critic = Critic().to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € (ê°œë³„ Actor/Criticë§Œ, GNNì€ ë³„ë„ë¡œ ê´€ë¦¬)
        self.optimizer = torch.optim.Adam(
            list(self.actor.parameters()) + 
            list(self.critic.parameters()), 
            lr=0.03
        )
        
        # ê²½í—˜ ë²„í¼
        self.memory = deque(maxlen=10000)
        
    def store_experience(self, experience: Dict):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append(experience)
    
    def get_all_params(self):
        """ê°œë³„ ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ë°˜í™˜ (GNN ì œì™¸)"""
        params = []
        params.extend(list(self.actor.parameters()))
        params.extend(list(self.critic.parameters()))
        return params
    
    def update_networks(self, shared_gnn):
        """PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ - ê³µìœ  GNN, ê°œë³„ Actor/Critic"""
        if len(self.memory) < 32:  # ìµœì†Œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
            return
        
        device = self.device
        
        # ê²½í—˜ ìƒ˜í”Œë§
        batch = random.sample(self.memory, min(32, len(self.memory)))
        
        # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        graph_data_list = [exp['graph_data'] for exp in batch]
        local_obs = torch.stack([exp['local_obs'] for exp in batch]).to(device)  # í…ì„œë“¤ì„ ìŠ¤íƒ
        actions = torch.tensor([exp['action'] for exp in batch]).to(device)
        old_log_probs = torch.tensor([exp['log_prob'] for exp in batch], dtype=torch.float32).to(device)
        rewards = torch.tensor([exp['reward'] for exp in batch], dtype=torch.float32).to(device)
        values = torch.tensor([exp['value'] for exp in batch], dtype=torch.float32).to(device)
        
        # GAE (Generalized Advantage Estimation)
        advantages = self._compute_gae(rewards, values).to(device)
        returns = (advantages + values).to(device)
        
        # PPO ì—…ë°ì´íŠ¸
        for _ in range(4):  # PPO ì—í­
            # ğŸ”¥ ê³µìœ  GNNìœ¼ë¡œ ê·¸ë˜í”„ ë°ì´í„° ì²˜ë¦¬ (ë°°ì¹˜ ë‹¨ìœ„)
            from torch_geometric.data import Batch
            batch_graphs = Batch.from_data_list(graph_data_list).to(device)
            node_embeddings = shared_gnn(batch_graphs)
            
            # ê° ìƒ˜í”Œì˜ ì—ì´ì „íŠ¸ ì„ë² ë”© ì¶”ì¶œ (ì²« ë²ˆì§¸ ë…¸ë“œê°€ ego agent)
            nodes_per_graph = len(graph_data_list[0].x)
            agent_embeddings = []
            global_embeddings = []
            
            for i in range(len(batch)):
                start_idx = i * nodes_per_graph
                # ego agent ì„ë² ë”© (Actorìš©)
                agent_emb = node_embeddings[start_idx + self.agent_id]  # ego agentëŠ” ìê¸° ìì‹ 
                agent_embeddings.append(agent_emb)
                
                # ì „ì—­ ì§‘ê³„ (Criticìš©) - ëª¨ë“  ì—ì´ì „íŠ¸ ë…¸ë“œë“¤ì˜ í‰ê· 
                # ì²« Nê°œ ë…¸ë“œê°€ ì—ì´ì „íŠ¸ë“¤ì´ë¼ê³  ê°€ì •
                num_agents = len([1 for exp in batch if exp])  # ë°°ì¹˜ ë‚´ ì—ì´ì „íŠ¸ ìˆ˜
                agent_nodes = node_embeddings[start_idx:start_idx + min(num_agents, nodes_per_graph)]
                if len(agent_nodes) > 0:
                    global_agg = agent_nodes.mean(dim=0)
                else:
                    global_agg = node_embeddings[start_idx]
                global_embeddings.append(global_agg)
            
            agent_embeddings = torch.stack(agent_embeddings)
            global_embeddings = torch.stack(global_embeddings)
            
            # ğŸ”¥ Actor: ë¡œì»¬ ê´€ì¸¡ + GNN ì§‘ê³„ ì •ë³´
            action_probs = self.actor(local_obs, agent_embeddings)
            dist = torch.distributions.Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy()
            
            # ğŸ”¥ Critic: ì „ì—­ ì§‘ê³„ ì •ë³´ë¡œ ê°’ í•¨ìˆ˜ ê³„ì‚°
            current_values = self.critic(global_embeddings).squeeze()
            
            # PPO ratio
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # Policy loss (clipped)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1-0.2, 1+0.2) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # Value loss
            value_loss = F.mse_loss(current_values, returns)
            
            # Entropy bonus
            entropy_loss = -entropy.mean()
            
            # Total loss
            total_loss = policy_loss + 0.5 * value_loss + 0.01 * entropy_loss
            
            # ğŸ”¥ ì—­ì „íŒŒ - GNN, Actor, Critic ëª¨ë‘ ì—…ë°ì´íŠ¸
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.get_all_params(), 0.5)
            self.optimizer.step()
    
    def _compute_gae(self, rewards, values, gamma=0.99, lam=0.95):
        """Generalized Advantage Estimation"""
        advantages = torch.zeros_like(rewards)
        gae = 0
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                delta = rewards[t] - values[t]
            else:
                delta = rewards[t] + gamma * values[t+1] - values[t]
            
            gae = delta + gamma * lam * gae
            advantages[t] = gae
        
        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)


@dataclass
class Agent2D:
    """2D ì›í˜• ì—ì´ì „íŠ¸"""
    id: int
    x: float
    y: float
    vx: float
    vy: float
    radius: float
    target_id: int  # ëª©í‘œ landmark ID
    max_speed: float
    
    def get_distance_to(self, other_x: float, other_y: float) -> float:
        return math.sqrt((self.x - other_x)**2 + (self.y - other_y)**2)
    
    def get_distance_to_agent(self, other: 'Agent2D') -> float:
        return self.get_distance_to(other.x, other.y)


@dataclass  
class Landmark2D:
    """2D ëª©í‘œ ì§€ì """
    id: int
    x: float
    y: float
    radius: float = 0.5


@dataclass
class Obstacle2D:
    """2D ì¥ì• ë¬¼"""
    id: int
    x: float
    y: float
    radius: float


# ì—”í‹°í‹° íƒ€ì… ë§¤í•‘ (ê¸°ì¡´ í”„ë¡œì íŠ¸ì™€ ë™ì¼)
ENTITY_TYPES = {"agent": 0, "landmark": 1, "obstacle": 2}


class BottleneckInforMARLEnv(gym.Env):
    """2D ë³‘ëª© í™˜ê²½ - InforMARL ê¸°ë°˜"""
    
    def __init__(self, 
                 num_agents: int = 6,
                 agent_radius: float = 0.5,
                 corridor_width: float = 20.0,
                 corridor_height: float = 10.0,
                 bottleneck_width: float = 1.2,  # ë” ì¢ê²Œ (ì´ì „ 1.8)
                 bottleneck_position: float = 10.0,
                 sensing_radius: float = 3.0,
                 max_timesteps: int = 300):
        
        super().__init__()
        
        self.num_agents = num_agents
        self.agent_radius = agent_radius
        self.corridor_width = corridor_width
        self.corridor_height = corridor_height
        self.bottleneck_width = bottleneck_width
        self.bottleneck_position = bottleneck_position
        self.sensing_radius = sensing_radius
        self.max_timesteps = max_timesteps
        
        # í–‰ë™ ê³µê°„: [ìœ„, ì•„ë˜, ì™¼ìª½, ì˜¤ë¥¸ìª½]
        self.action_space = spaces.Discrete(4)
        
        self.agents: List[Agent2D] = []
        self.landmarks: List[Landmark2D] = []
        self.obstacles: List[Obstacle2D] = []
        self.informarl_agents: List[InforMARLAgent] = []
        
        # ê³µìœ  GNNê³¼ ì˜µí‹°ë§ˆì´ì €
        self.shared_gnn = None
        self.gnn_optimizer = None
        
        self.timestep = 0
        self.success_count = 0
        self.collision_count = 0
        
        # ë Œë”ë§ ê´€ë ¨
        self.fig = None
        self.ax = None
        self.render_mode = None
        
    def reset(self) -> List[Data]:
        """í™˜ê²½ ë¦¬ì…‹"""
        self.timestep = 0
        self.success_count = 0
        self.collision_count = 0
        self.agents = []
        self.landmarks = []
        self.obstacles = []
        self.informarl_agents = []
        
        # ì¥ì• ë¬¼ ì„¤ì • ë¨¼ì € ì •ì˜
        # ì—ì´ì „íŠ¸ ì§€ë¦„ = agent_radius * 2 = 1.0
        # ì¥ì• ë¬¼ ê°„ê²©ì„ ì—ì´ì „íŠ¸ ì§€ë¦„ì˜ 1/3ë¡œ ì„¤ì •í•˜ì—¬ ì´˜ì´˜í•˜ê²Œ
        obstacle_spacing = (self.agent_radius * 2) / 3  # â‰ˆ 0.33
        obstacle_radius = obstacle_spacing / 2  # ì¥ì• ë¬¼ ë°˜ì§€ë¦„
        wall_margin = obstacle_radius * 3  # ë²½ì—ì„œ ì¶©ë¶„í•œ ê±°ë¦¬ í™•ë³´
        
        # ëª©í‘œ ì§€ì  ìƒì„± (ë²½ ì¥ì• ë¬¼ì—ì„œ ì¶©ë¶„íˆ ë–¨ì–´ëœ¨ë¦¬ê¸°)
        for i in range(self.num_agents):
            if i % 2 == 0:  # L->R
                target_x = np.random.uniform(self.corridor_width - 3.0, self.corridor_width - wall_margin)
            else:  # R->L
                target_x = np.random.uniform(wall_margin, 3.0)
            
            target_y = np.random.uniform(wall_margin, self.corridor_height - wall_margin)
            
            landmark = Landmark2D(id=i, x=target_x, y=target_y)
            self.landmarks.append(landmark)
        
        # ì¥ì• ë¬¼ ìƒì„± - í™˜ê²½ì˜ ëª¨ë“  ë²½ì„ ì¥ì• ë¬¼ ë…¸ë“œë¡œ ë‘˜ëŸ¬ì‹¸ê¸°
        
        obstacle_id = 0
        
        # 1. ìƒë‹¨ ê²½ê³„ë²½ (ì „ì²´ ë„ˆë¹„)
        num_top_obstacles = int(self.corridor_width / obstacle_spacing) + 1
        for i in range(num_top_obstacles):
            x_pos = i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=x_pos,
                y=self.corridor_height - obstacle_radius,  # ìƒë‹¨ ê²½ê³„
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # 2. í•˜ë‹¨ ê²½ê³„ë²½ (ì „ì²´ ë„ˆë¹„)
        for i in range(num_top_obstacles):
            x_pos = i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=x_pos,
                y=obstacle_radius,  # í•˜ë‹¨ ê²½ê³„
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # 3. ì¢Œì¸¡ ê²½ê³„ë²½ (ì „ì²´ ë†’ì´)
        num_left_obstacles = int(self.corridor_height / obstacle_spacing) + 1
        for i in range(num_left_obstacles):
            y_pos = i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=obstacle_radius,  # ì¢Œì¸¡ ê²½ê³„
                y=y_pos,
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # 4. ìš°ì¸¡ ê²½ê³„ë²½ (ì „ì²´ ë†’ì´)
        for i in range(num_left_obstacles):
            y_pos = i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=self.corridor_width - obstacle_radius,  # ìš°ì¸¡ ê²½ê³„
                y=y_pos,
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # 5. ë³‘ëª© êµ¬ê°„ ë²½ë“¤ - í†µë¡œëŠ” ì™„ì „íˆ ì—´ì–´ë‘ 
        center_y = self.corridor_height / 2
        wall_length = 2.0
        num_bottleneck_obstacles = int(wall_length / obstacle_spacing) + 1
        
        # ë³‘ëª© í†µë¡œì˜ ìƒí•˜ ê²½ê³„ ê³„ì‚° (ì—¬ìœ  ê³µê°„ ì¶”ê°€)
        passage_margin = obstacle_radius * 2  # í†µë¡œ ì£¼ë³€ì— ì¶©ë¶„í•œ ì—¬ìœ  ê³µê°„
        passage_top = center_y + self.bottleneck_width/2 + passage_margin
        passage_bottom = center_y - self.bottleneck_width/2 - passage_margin
        
        # ë³‘ëª© ìœ„ìª½ ë²½
        upper_wall_y = center_y + self.bottleneck_width/2 + obstacle_radius
        for i in range(num_bottleneck_obstacles):
            x_pos = self.bottleneck_position - wall_length/2 + i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=x_pos,
                y=upper_wall_y,
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # ë³‘ëª© ì•„ë˜ìª½ ë²½
        lower_wall_y = center_y - self.bottleneck_width/2 - obstacle_radius
        for i in range(num_bottleneck_obstacles):
            x_pos = self.bottleneck_position - wall_length/2 + i * obstacle_spacing
            obstacle = Obstacle2D(
                id=obstacle_id,
                x=x_pos,
                y=lower_wall_y,
                radius=obstacle_radius
            )
            self.obstacles.append(obstacle)
            obstacle_id += 1
        
        # 6. ë³‘ëª© ì…êµ¬ ì¢Œì¸¡ ë²½ (ì¢Œì¸¡ì—ì„œ ë³‘ëª©ê¹Œì§€, í†µë¡œ ë¶€ë¶„ì€ ì œì™¸)
        left_wall_x = self.bottleneck_position - wall_length/2 - obstacle_radius
        
        # í•˜ë‹¨ ê²½ê³„ì—ì„œ ë³‘ëª© í†µë¡œ ì•„ë˜ê¹Œì§€
        num_bottom_obstacles = int((passage_bottom - obstacle_radius) / obstacle_spacing)
        for i in range(num_bottom_obstacles):
            y_pos = obstacle_radius + i * obstacle_spacing
            # í†µë¡œ ì—¬ìœ  ê³µê°„ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ë” ì—„ê²©í•œ ì¡°ê±´
            if y_pos < passage_bottom - passage_margin:
                obstacle = Obstacle2D(
                    id=obstacle_id,
                    x=left_wall_x,
                    y=y_pos,
                    radius=obstacle_radius
                )
                self.obstacles.append(obstacle)
                obstacle_id += 1
        
        # ë³‘ëª© í†µë¡œ ìœ„ì—ì„œ ìƒë‹¨ ê²½ê³„ê¹Œì§€
        start_y = passage_top + passage_margin  # ì—¬ìœ  ê³µê°„ ì¶”ê°€
        num_top_obstacles = int((self.corridor_height - start_y) / obstacle_spacing)
        for i in range(num_top_obstacles):
            y_pos = start_y + i * obstacle_spacing
            if y_pos < self.corridor_height - obstacle_radius:
                obstacle = Obstacle2D(
                    id=obstacle_id,
                    x=left_wall_x,
                    y=y_pos,
                    radius=obstacle_radius
                )
                self.obstacles.append(obstacle)
                obstacle_id += 1
        
        # 7. ë³‘ëª© ì…êµ¬ ìš°ì¸¡ ë²½ (ë³‘ëª©ì—ì„œ ìš°ì¸¡ê¹Œì§€, í†µë¡œ ë¶€ë¶„ì€ ì œì™¸)
        right_wall_x = self.bottleneck_position + wall_length/2 + obstacle_radius
        
        # í•˜ë‹¨ ê²½ê³„ì—ì„œ ë³‘ëª© í†µë¡œ ì•„ë˜ê¹Œì§€ (ìš°ì¸¡ë²½)
        for i in range(num_bottom_obstacles):
            y_pos = obstacle_radius + i * obstacle_spacing
            if y_pos < passage_bottom - passage_margin:  # ì—¬ìœ  ê³µê°„ê³¼ ê²¹ì¹˜ì§€ ì•Šë„ë¡
                obstacle = Obstacle2D(
                    id=obstacle_id,
                    x=right_wall_x,
                    y=y_pos,
                    radius=obstacle_radius
                )
                self.obstacles.append(obstacle)
                obstacle_id += 1
        
        # ë³‘ëª© í†µë¡œ ìœ„ì—ì„œ ìƒë‹¨ ê²½ê³„ê¹Œì§€ (ìš°ì¸¡ë²½)
        for i in range(num_top_obstacles):
            y_pos = start_y + i * obstacle_spacing
            if y_pos < self.corridor_height - obstacle_radius:
                obstacle = Obstacle2D(
                    id=obstacle_id,
                    x=right_wall_x,
                    y=y_pos,
                    radius=obstacle_radius
                )
                self.obstacles.append(obstacle)
                obstacle_id += 1
        
        # ì—ì´ì „íŠ¸ ìƒì„± (ë²½ ì¥ì• ë¬¼ì—ì„œ ì¶©ë¶„íˆ ë–¨ì–´ëœ¨ë¦¬ê¸°)
        for i in range(self.num_agents):
            if i % 2 == 0:  # L->R
                start_x = np.random.uniform(wall_margin, 3.0)
            else:  # R->L
                start_x = np.random.uniform(self.corridor_width - 3.0, self.corridor_width - wall_margin)
            
            start_y = np.random.uniform(wall_margin, self.corridor_height - wall_margin)
            max_speed = np.random.uniform(1.0, 2.0)
            
            agent = Agent2D(
                id=i, x=start_x, y=start_y, vx=0.0, vy=0.0,
                radius=self.agent_radius, target_id=i, max_speed=max_speed
            )
            self.agents.append(agent)
            
            # InforMARL ì—ì´ì „íŠ¸ ìƒì„±
            informarl_agent = InforMARLAgent(agent_id=i)
            self.informarl_agents.append(informarl_agent)
        
        # ê³µìœ  GNN ì´ˆê¸°í™” (ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì˜ GNNì„ ê³µìœ ë¡œ ì‚¬ìš©)
        if self.shared_gnn is None:
            device = self.informarl_agents[0].device
            self.shared_gnn = GraphNeuralNetwork().to(device)
            self.gnn_optimizer = torch.optim.Adam(self.shared_gnn.parameters(), lr=0.03)
        
        return self._get_graph_observations()
    
    def _get_graph_observations(self) -> List[Data]:
        """InforMARL ë°©ì‹ ê·¸ë˜í”„ ê´€ì¸¡ ìƒì„±"""
        observations = []
        
        # ê° ì—ì´ì „íŠ¸ë§ˆë‹¤ ìì‹ ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ê·¸ë˜í”„ ìƒì„±
        for ego_agent in self.agents:
            node_features = []
            entity_types = []
            all_entities = []
            
            # 1. ì—ì´ì „íŠ¸ ë…¸ë“œë“¤
            for agent in self.agents:
                # ìƒëŒ€ ìœ„ì¹˜/ì†ë„/ëª©í‘œ ê³„ì‚° (sensing_radiusë¡œ ì •ê·œí™”)
                rel_x = (agent.x - ego_agent.x) / self.sensing_radius
                rel_y = (agent.y - ego_agent.y) / self.sensing_radius
                rel_vx = (agent.vx - ego_agent.vx) / agent.max_speed
                rel_vy = (agent.vy - ego_agent.vy) / agent.max_speed
                
                # ëª©í‘œ ìœ„ì¹˜
                target = self.landmarks[agent.target_id]
                rel_goal_x = (target.x - ego_agent.x) / self.sensing_radius
                rel_goal_y = (target.y - ego_agent.y) / self.sensing_radius
                
                features = [rel_x, rel_y, rel_vx, rel_vy, rel_goal_x, rel_goal_y]
                node_features.append(features)
                entity_types.append(ENTITY_TYPES["agent"])
                all_entities.append(('agent', agent))
            
            # 2. ëª©í‘œ ì§€ì  ë…¸ë“œë“¤
            for landmark in self.landmarks:
                rel_x = (landmark.x - ego_agent.x) / self.sensing_radius
                rel_y = (landmark.y - ego_agent.y) / self.sensing_radius
                
                features = [rel_x, rel_y, 0.0, 0.0, rel_x, rel_y]  # ëª©í‘œëŠ” ì •ì§€, ëª©í‘œ=ìê¸°ìœ„ì¹˜
                node_features.append(features)
                entity_types.append(ENTITY_TYPES["landmark"])
                all_entities.append(('landmark', landmark))
            
            # 3. ì¥ì• ë¬¼ ë…¸ë“œë“¤
            for obstacle in self.obstacles:
                rel_x = (obstacle.x - ego_agent.x) / self.sensing_radius
                rel_y = (obstacle.y - ego_agent.y) / self.sensing_radius
                
                features = [rel_x, rel_y, 0.0, 0.0, rel_x, rel_y]  # ì¥ì• ë¬¼ì€ ì •ì§€
                node_features.append(features)
                entity_types.append(ENTITY_TYPES["obstacle"])
                all_entities.append(('obstacle', obstacle))
            
            # 4. ì—£ì§€ ìƒì„± (ë…¼ë¬¸ì˜ ë°©í–¥ì„± ê·œì¹™ì— ë”°ë¼)
            # - non-agent â†’ agent: ë‹¨ë°©í–¥ (landmark, obstacle â†’ agent)
            # - agent â†” agent: ì–‘ë°©í–¥
            edge_index = []
            edge_attr = []
            
            for i, (type_i, entity_i) in enumerate(all_entities):
                for j, (type_j, entity_j) in enumerate(all_entities):
                    if i != j:
                        # ego_agentë¡œë¶€í„° ë‘ ì—”í‹°í‹°ê¹Œì§€ì˜ ê±°ë¦¬ ì²´í¬
                        if type_i == 'agent':
                            dist_i = ego_agent.get_distance_to_agent(entity_i)
                        else:
                            dist_i = ego_agent.get_distance_to(entity_i.x, entity_i.y)
                            
                        if type_j == 'agent':
                            dist_j = ego_agent.get_distance_to_agent(entity_j)
                        else:
                            dist_j = ego_agent.get_distance_to(entity_j.x, entity_j.y)
                        
                        # ë‘ ì—”í‹°í‹° ëª¨ë‘ ego_agentì˜ ì„¼ì‹± ë°˜ê²½ ë‚´ì— ìˆì–´ì•¼ í•¨
                        if dist_i <= self.sensing_radius and dist_j <= self.sensing_radius:
                            # ì—£ì§€ ë°©í–¥ì„± ê·œì¹™ ì ìš©
                            should_connect = False
                            
                            if type_i == 'agent' and type_j == 'agent':
                                # agent â†” agent: ì–‘ë°©í–¥ ì—°ê²°
                                should_connect = True
                            elif type_i in ['landmark', 'obstacle'] and type_j == 'agent':
                                # non-agent â†’ agent: ë‹¨ë°©í–¥ ì—°ê²°
                                should_connect = True
                            elif type_i == 'agent' and type_j in ['landmark', 'obstacle']:
                                # agent â†’ non-agent: ì—°ê²°í•˜ì§€ ì•ŠìŒ (ë…¼ë¬¸ ê·œì¹™)
                                should_connect = False
                            else:
                                # non-agent â†’ non-agent: ì—°ê²°í•˜ì§€ ì•ŠìŒ
                                should_connect = False
                            
                            if should_connect:
                                edge_index.append([i, j])
                                
                                # ì—£ì§€ íŠ¹ì§•: ë‘ ì—”í‹°í‹° ê°„ ê±°ë¦¬
                                if type_i == 'agent' and type_j == 'agent':
                                    edge_dist = entity_i.get_distance_to_agent(entity_j)
                                elif type_i == 'agent':
                                    edge_dist = entity_i.get_distance_to(entity_j.x, entity_j.y)
                                elif type_j == 'agent':
                                    edge_dist = entity_j.get_distance_to(entity_i.x, entity_i.y)
                                else:
                                    dx = entity_i.x - entity_j.x
                                    dy = entity_i.y - entity_j.y
                                    edge_dist = math.sqrt(dx*dx + dy*dy)
                                
                                edge_attr.append([edge_dist / self.sensing_radius])  # ì •ê·œí™”
            
            # ìµœì†Œ ì—°ê²° ë³´ì¥
            if not edge_index:
                edge_index = [[0, 0]]
                edge_attr = [[0.0]]
            
            # ê·¸ë˜í”„ ë°ì´í„° ìƒì„±
            graph_data = Data(
                x=torch.tensor(node_features, dtype=torch.float32),
                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),
                edge_attr=torch.tensor(edge_attr, dtype=torch.float32),
                entity_type=torch.tensor(entity_types, dtype=torch.long)
            )
            
            observations.append(graph_data)
        
        return observations
    
    def step(self, actions: List[int] = None):
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        self.timestep += 1
        
        # ê·¸ë˜í”„ ê´€ì¸¡
        graph_obs = self._get_graph_observations()
        
        # í–‰ë™ ì„ íƒ (ë°°ì¹˜ ì²˜ë¦¬)
        if actions is None:
            actions, log_probs, values = self._get_batch_actions(graph_obs, training=True)
        else:
            log_probs = [0.0] * len(actions)
            values = [0.0] * len(actions)
        
        # í–‰ë™ ì‹¤í–‰
        for i, action in enumerate(actions):
            self._execute_action(i, action)
        
        # ë¬¼ë¦¬ ì—…ë°ì´íŠ¸
        self._update_positions()
        
        # ë³´ìƒ ê³„ì‚°
        rewards = self._calculate_rewards()
        
        # ê²½í—˜ ì €ì¥
        if actions is not None:
            for i, (action, log_prob, value, reward) in enumerate(zip(actions, log_probs, values, rewards)):
                obs = self._get_local_observation(i)
                experience = {
                    'graph_data': graph_obs[i],
                    'local_obs': torch.tensor(obs, dtype=torch.float32),  # í…ì„œë¡œ ë³€í™˜
                    'action': action,
                    'log_prob': log_prob,
                    'value': value,
                    'reward': reward
                }
                self.informarl_agents[i].store_experience(experience)
        
        new_obs = self._get_graph_observations()
        done = self._is_done()
        info = self._get_info()
        
        return new_obs, rewards, done, info
    
    def _get_batch_actions(self, graph_observations: List[Data], training: bool = True):
        """ë°°ì¹˜ í–‰ë™ ì„ íƒ - InforMARL ë°©ì‹"""
        device = self.informarl_agents[0].device
        
        # ë°°ì¹˜ ê·¸ë˜í”„ ìƒì„±
        batch_graphs = Batch.from_data_list(graph_observations).to(device)
        
        # ê³µìœ  GNNìœ¼ë¡œ ë…¸ë“œ ì„ë² ë”© ê³„ì‚°
        node_embeddings = self.shared_gnn(batch_graphs)
        
        # ì—ì´ì „íŠ¸ë³„ ì •ë³´ ì¶”ì¶œ
        nodes_per_graph = len(graph_observations[0].x)
        num_agents_in_graph = self.num_agents
        
        actions = []
        log_probs = []
        values = []
        
        if training:
            # í•™ìŠµ ì‹œ: Actor + Critic ëª¨ë‘ ì‚¬ìš©
            
            # ê° ì—ì´ì „íŠ¸ì˜ ê°œë³„ ì„ë² ë”© ì¶”ì¶œ (Actorìš©)
            agent_embeddings = []
            for i in range(self.num_agents):
                start_idx = i * nodes_per_graph
                # ego_agentëŠ” í•­ìƒ ì²« ë²ˆì§¸ ë…¸ë“œ (ì¸ë±ìŠ¤ i)
                agent_emb = node_embeddings[start_idx + i]
                agent_embeddings.append(agent_emb)
            
            # Criticìš© ì „ì—­ ì§‘ê³„
            # ê° ê·¸ë˜í”„ì—ì„œ ì—ì´ì „íŠ¸ ë…¸ë“œë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ í‰ê· 
            graph_embeddings = []
            for i in range(self.num_agents):
                start_idx = i * nodes_per_graph
                # ì²« num_agentsê°œ ë…¸ë“œê°€ ì—ì´ì „íŠ¸ë“¤
                agent_nodes = node_embeddings[start_idx:start_idx + num_agents_in_graph]
                graph_agg = agent_nodes.mean(dim=0)  # ì—ì´ì „íŠ¸ë“¤ì˜ í‰ê· 
                graph_embeddings.append(graph_agg)
            
            global_agg = torch.stack(graph_embeddings)  # [num_agents, hidden_dim]
            global_values = self.informarl_agents[0].critic(global_agg)
            
            for i, agent in enumerate(self.informarl_agents):
                # ë¡œì»¬ ê´€ì¸¡
                local_obs = torch.tensor(self._get_local_observation(i), dtype=torch.float32).unsqueeze(0).to(device)
                
                # Actor: ë¡œì»¬ ê´€ì¸¡ + ì§‘ê³„ ì •ë³´
                agg_info = agent_embeddings[i].unsqueeze(0)
                action_probs = agent.actor(local_obs, agg_info)
                
                # í™•ë¥ ì  í–‰ë™ ì„ íƒ
                action_dist = torch.distributions.Categorical(action_probs)
                action = action_dist.sample()
                log_prob = action_dist.log_prob(action)
                
                actions.append(action.item())
                log_probs.append(log_prob.item())
                values.append(global_values[i].item())
        else:
            # í‰ê°€ ì‹œ: Actorë§Œ ì‚¬ìš©
            agent_embeddings = []
            for i in range(self.num_agents):
                start_idx = i * nodes_per_graph
                agent_emb = node_embeddings[start_idx + i]
                agent_embeddings.append(agent_emb)
            
            for i, agent in enumerate(self.informarl_agents):
                local_obs = torch.tensor(self._get_local_observation(i), dtype=torch.float32).unsqueeze(0).to(device)
                agg_info = agent_embeddings[i].unsqueeze(0)
                action_probs = agent.actor(local_obs, agg_info)
                
                # ê²°ì •ì  í–‰ë™ ì„ íƒ
                action = torch.argmax(action_probs, dim=1)
                actions.append(action.item())
                log_probs.append(0.0)
                values.append(0.0)
        
        return actions, log_probs, values
    
    def _get_local_observation(self, agent_id: int) -> List[float]:
        """ì—ì´ì „íŠ¸ì˜ ë¡œì»¬ ê´€ì¸¡ (ë…¼ë¬¸ì˜ o(i))"""
        agent = self.agents[agent_id]
        target = self.landmarks[agent.target_id]
        
        return [
            agent.x / self.sensing_radius,    # sensing_radiusë¡œ ì •ê·œí™”ëœ ìœ„ì¹˜
            agent.y / self.sensing_radius,
            agent.vx / agent.max_speed,       # ì •ê·œí™”ëœ ì†ë„
            agent.vy / agent.max_speed,
            (target.x - agent.x) / self.sensing_radius,  # ìƒëŒ€ ëª©í‘œ ìœ„ì¹˜
            (target.y - agent.y) / self.sensing_radius
        ]
    
    def _execute_action(self, agent_id: int, action: int):
        """í–‰ë™ ì‹¤í–‰ - ì¶©ëŒ í˜ë„í‹° ì ìš©"""
        agent = self.agents[agent_id]
        
        # ì¶©ëŒ í˜ë„í‹° ì²´í¬
        collision_penalty = getattr(agent, 'collision_penalty_timer', 0)
        if collision_penalty > 0:
            speed = agent.max_speed * 0.2  # ì¶©ëŒ í›„ ëŠë¦° ì†ë„
            agent.collision_penalty_timer = collision_penalty - 1
        else:
            speed = agent.max_speed * 0.5  # ì¼ë°˜ ì†ë„
        
        if action == 0:  # ìœ„
            agent.vy = min(agent.vy + speed, agent.max_speed)
        elif action == 1:  # ì•„ë˜
            agent.vy = max(agent.vy - speed, -agent.max_speed)
        elif action == 2:  # ì™¼ìª½
            agent.vx = max(agent.vx - speed, -agent.max_speed)
        elif action == 3:  # ì˜¤ë¥¸ìª½
            agent.vx = min(agent.vx + speed, agent.max_speed)
        
        # ì†ë„ ê°ì‡  (ì¶©ëŒ í˜ë„í‹° ì¤‘ì´ë©´ ë” ê°•í•˜ê²Œ)
        decay_factor = 0.7 if collision_penalty > 0 else 0.9
        agent.vx *= decay_factor
        agent.vy *= decay_factor
    
    def _update_positions(self):
        """ìœ„ì¹˜ ì—…ë°ì´íŠ¸ - ê°•í™”ëœ ì¶©ëŒ ì²˜ë¦¬"""
        dt = 0.1
        
        for agent in self.agents:
            new_x = agent.x + agent.vx * dt
            new_y = agent.y + agent.vy * dt
            
            # í†µí•©ëœ ì¶©ëŒ ì²´í¬ (ê²½ê³„, ë²½, ì¥ì• ë¬¼, ì—ì´ì „íŠ¸ ëª¨ë‘ í¬í•¨)
            collision_info = self._check_collision_detailed(agent.id, new_x, new_y)
            
            if not collision_info['has_collision']:
                agent.x = new_x
                agent.y = new_y
            else:
                # ì¶©ëŒ ì‹œ ê°•í™”ëœ ì²˜ë¦¬
                self._handle_collision(agent, collision_info)
                self.collision_count += 1
    
    def _check_collision_detailed(self, agent_id: int, new_x: float, new_y: float) -> Dict[str, Any]:
        """ìƒì„¸ ì¶©ëŒ ì •ë³´ ì²´í¬"""
        agent_radius = self.agents[agent_id].radius
        collision_info = {
            'has_collision': False,
            'collision_type': None,
            'collision_entity': None,
            'penetration_depth': 0.0
        }
        
        # 1. í™˜ê²½ ê²½ê³„ ì²´í¬
        if (new_x - agent_radius < 0 or new_x + agent_radius > self.corridor_width or
            new_y - agent_radius < 0 or new_y + agent_radius > self.corridor_height):
            collision_info.update({
                'has_collision': True,
                'collision_type': 'boundary',
                'penetration_depth': max(
                    max(0, (agent_radius - new_x)),  # ì™¼ìª½ ê²½ê³„
                    max(0, (new_x + agent_radius - self.corridor_width)),  # ì˜¤ë¥¸ìª½ ê²½ê³„
                    max(0, (agent_radius - new_y)),  # ì•„ë˜ ê²½ê³„
                    max(0, (new_y + agent_radius - self.corridor_height))  # ìœ„ ê²½ê³„
                )
            })
            return collision_info
        
        # 2. ë³‘ëª© ë²½ ì¶©ëŒ ì²´í¬
        if not self._can_pass_through_bottleneck(new_x, new_y, agent_radius):
            collision_info.update({
                'has_collision': True,
                'collision_type': 'bottleneck_wall'
            })
            return collision_info
        
        # 3. ì¥ì• ë¬¼ ì¶©ëŒ ì²´í¬
        for obstacle in self.obstacles:
            dist = math.sqrt((new_x - obstacle.x)**2 + (new_y - obstacle.y)**2)
            min_dist = agent_radius + obstacle.radius
            if dist < min_dist:
                collision_info.update({
                    'has_collision': True,
                    'collision_type': 'obstacle',
                    'collision_entity': obstacle,
                    'penetration_depth': min_dist - dist
                })
                return collision_info
        
        # 4. ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì™€ì˜ ì¶©ëŒ ì²´í¬
        for i, other in enumerate(self.agents):
            if i != agent_id:
                dist = math.sqrt((new_x - other.x)**2 + (new_y - other.y)**2)
                min_dist = agent_radius + other.radius
                if dist < min_dist:
                    collision_info.update({
                        'has_collision': True,
                        'collision_type': 'agent',
                        'collision_entity': other,
                        'penetration_depth': min_dist - dist
                    })
                    return collision_info
        
        return collision_info
    
    def _handle_collision(self, agent: Agent2D, collision_info: Dict[str, Any]):
        """ê°•í™”ëœ ì¶©ëŒ ì²˜ë¦¬"""
        collision_type = collision_info['collision_type']
        
        # ì¦‰ì‹œ ì†ë„ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì¶”ê°€ ì¹¨íˆ¬ ë°©ì§€
        agent.vx = 0.0
        agent.vy = 0.0
        
        # ì¶©ëŒ íƒ€ì…ë³„ ì¶”ê°€ ì²˜ë¦¬
        if collision_type == 'boundary':
            # ê²½ê³„ ì¶©ëŒ: ê²½ê³„ì—ì„œ ë°€ì–´ë‚´ê¸°
            margin = agent.radius + 0.1
            if agent.x < margin:
                agent.x = margin
            elif agent.x > self.corridor_width - margin:
                agent.x = self.corridor_width - margin
            if agent.y < margin:
                agent.y = margin
            elif agent.y > self.corridor_height - margin:
                agent.y = self.corridor_height - margin
                
        elif collision_type in ['obstacle', 'agent']:
            # ì¥ì• ë¬¼/ì—ì´ì „íŠ¸ ì¶©ëŒ: ì¶©ëŒ ì—”í‹°í‹°ë¡œë¶€í„° ë°€ì–´ë‚´ê¸°
            entity = collision_info['collision_entity']
            penetration = collision_info.get('penetration_depth', 0.0)
            
            if penetration > 0.01:  # ì¶©ë¶„í•œ ì¹¨íˆ¬ê°€ ìˆì„ ë•Œë§Œ
                # ì¶©ëŒ ë°©í–¥ ê³„ì‚°
                dx = agent.x - entity.x
                dy = agent.y - entity.y
                dist = math.sqrt(dx*dx + dy*dy)
                
                if dist > 0.01:  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                    # ì •ê·œí™”ëœ ë°©í–¥ìœ¼ë¡œ ë°€ì–´ë‚´ê¸°
                    push_distance = penetration + 0.1  # ì•½ê°„ ì—¬ìœ ë¥¼ ë‘ 
                    agent.x += (dx / dist) * push_distance
                    agent.y += (dy / dist) * push_distance
        
        # ì¶©ëŒí•œ ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒ ìŠ¤í…ì—ì„œ ì ì‹œ ëŠë¦¬ê²Œ ì›€ì§ì„
        agent.collision_penalty_timer = getattr(agent, 'collision_penalty_timer', 0) + 3
    
    
    
    def _can_pass_through_bottleneck(self, x: float, y: float, radius: float) -> bool:
        """ë³‘ëª© êµ¬ê°„ í†µê³¼ ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬"""
        center_y = self.corridor_height / 2
        bottleneck_x = self.bottleneck_position
        
        # ë³‘ëª© êµ¬ì—­ì´ ì•„ë‹ˆë©´ í†µê³¼ ê°€ëŠ¥
        if abs(x - bottleneck_x) > 1.0:
            return True
        
        # ë³‘ëª© êµ¬ì—­ ë‚´ì—ì„œëŠ” í†µë¡œ í­ ì²´í¬ (ì—ì´ì „íŠ¸ ë°˜ì§€ë¦„ ê³ ë ¤)
        passage_top = center_y + self.bottleneck_width / 2
        passage_bottom = center_y - self.bottleneck_width / 2
        
        # ì—ì´ì „íŠ¸ê°€ í†µë¡œ ì•ˆì— ì™„ì „íˆ ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ”ì§€ ì²´í¬
        agent_top = y + radius
        agent_bottom = y - radius
        
        return (agent_bottom >= passage_bottom and agent_top <= passage_top)
    
    def _calculate_rewards(self) -> List[float]:
        """ê°œì„ ëœ ë³´ìƒ ê³„ì‚° - ì¢Œìš° ë°˜ë³µ ì´ë™ ë°©ì§€"""
        rewards = []
        
        for agent in self.agents:
            reward = 0.0
            target = self.landmarks[agent.target_id]
            distance = agent.get_distance_to(target.x, target.y)
            
            # ëª©í‘œ ë„ë‹¬ - í° ë³´ìƒ
            if distance < target.radius:
                reward += 100.0
                self.success_count += 1
                rewards.append(reward)
                continue
            
            # 1. ê±°ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ë³´ìƒ (ìŒìˆ˜ë¡œ ì‹œì‘í•´ì„œ ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ëœ ë‚˜ì¨)
            reward -= distance * 0.1
            
            # 2. ëª©í‘œ ë°©í–¥ ì´ë™ ë³´ìƒ (ê°€ì¥ ì¤‘ìš”!)
            target_direction = np.array([target.x - agent.x, target.y - agent.y])
            target_distance = np.linalg.norm(target_direction)
            
            if target_distance > 0.1:  # ëª©í‘œì— ì¶©ë¶„íˆ ë©€ ë•Œë§Œ
                target_direction = target_direction / target_distance  # ì •ê·œí™”
                agent_velocity = np.array([agent.vx, agent.vy])
                velocity_magnitude = np.linalg.norm(agent_velocity)
                
                if velocity_magnitude > 0.05:  # ì›€ì§ì´ê³  ìˆì„ ë•Œë§Œ
                    agent_direction = agent_velocity / velocity_magnitude
                    # ëª©í‘œ ë°©í–¥ìœ¼ë¡œì˜ ì†ë„ ì„±ë¶„ (ë‚´ì )
                    direction_alignment = np.dot(agent_direction, target_direction)
                    reward += direction_alignment * velocity_magnitude * 2.0
                else:
                    # ì •ì§€í•´ìˆìœ¼ë©´ ì•½ê°„ì˜ í˜ë„í‹°
                    reward -= 0.1
            
            # 3. ê±°ë¦¬ ê°œì„  ë³´ìƒ (ì´ì „ë³´ë‹¤ ê°ì†Œ)
            prev_dist = getattr(agent, 'prev_distance', distance)
            if distance < prev_dist:
                improvement = prev_dist - distance
                reward += improvement * 10.0  # ë” í° ë³´ìƒ
            agent.prev_distance = distance
            
            # 4. ë°˜ë³µ ì›€ì§ì„ í˜ë„í‹°
            prev_positions = getattr(agent, 'position_history', [])
            current_pos = (agent.x, agent.y)
            
            # ìµœê·¼ ìœ„ì¹˜ ê¸°ë¡ (ìµœëŒ€ 10ê°œ)
            prev_positions.append(current_pos)
            if len(prev_positions) > 10:
                prev_positions.pop(0)
            agent.position_history = prev_positions
            
            # ê°™ì€ ìœ„ì¹˜ ë°˜ë³µ ì²´í¬
            if len(prev_positions) >= 5:
                recent_positions = prev_positions[-5:]
                position_variance = np.var([pos[0] for pos in recent_positions]) + np.var([pos[1] for pos in recent_positions])
                if position_variance < 0.1:  # ê±°ì˜ ê°™ì€ ìë¦¬
                    reward -= 2.0  # ì •ì²´ í˜ë„í‹°
            
            # 5. ì¶©ëŒ í˜ë„í‹° - ì¶©ëŒ ì‹œ í° ë§ˆì´ë„ˆìŠ¤ ë³´ìƒ
            collision_penalty = getattr(agent, 'collision_penalty_timer', 0)
            if collision_penalty > 0:
                reward -= 5.0  # ì¶©ëŒ ì‹œ í° í˜ë„í‹°
            
            # 6. ì‹œê°„ í˜ë„í‹° (ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´)
            reward -= 0.01  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ì‘ì€ ì‹œê°„ í˜ë„í‹°
            
            rewards.append(reward)
        
        return rewards
    
    def _is_done(self) -> bool:
        """ì¢…ë£Œ ì¡°ê±´"""
        if self.timestep >= self.max_timesteps:
            return True
        
        # ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ëª©í‘œ ë„ë‹¬
        for agent in self.agents:
            target = self.landmarks[agent.target_id]
            if agent.get_distance_to(target.x, target.y) >= target.radius:
                return False
        return True
    
    def _get_info(self) -> Dict[str, Any]:
        """ì •ë³´ ë°˜í™˜"""
        return {
            "timestep": self.timestep,
            "success_count": self.success_count,
            "collision_count": self.collision_count,
            "success_rate": self.success_count / max(1, self.num_agents),
            "avg_time_ratio": self.timestep / self.max_timesteps
        }
    
    def train_agents(self, num_episodes: int = 100):
        """ì‹¤ì œ í•™ìŠµì´ í¬í•¨ëœ í•¨ìˆ˜"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            observations = self.reset()
            episode_reward = 0
            
            for step in range(self.max_timesteps):
                observations, rewards, done, info = self.step()
                episode_reward += sum(rewards)
                
                # ğŸ”¥ ë§¤ NìŠ¤í…ë§ˆë‹¤ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
                if step % 10 == 0:
                    for agent in self.informarl_agents:
                        agent.update_networks(self.shared_gnn)
                    # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                    self.gnn_optimizer.step()
                    self.gnn_optimizer.zero_grad()
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
            
            # ğŸ”¥ ì—í”¼ì†Œë“œ ëì— í•œ ë²ˆ ë” ì—…ë°ì´íŠ¸
            for agent in self.informarl_agents:
                agent.update_networks(self.shared_gnn)
            # ê³µìœ  GNN ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            self.gnn_optimizer.step()
            self.gnn_optimizer.zero_grad()
                
            if episode % 10 == 0:
                avg_reward = np.mean(episode_rewards[-10:])
                print(f"Episode {episode}: Avg Reward = {avg_reward:.2f}, "
                      f"Success Rate = {info['success_rate']:.2f}")
        
        return episode_rewards
    
    def render(self, mode='human'):
        """í™˜ê²½ ë Œë”ë§"""
        if self.fig is None:
            self.fig, self.ax = plt.subplots(figsize=(14, 8))
            plt.ion()  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        
        self.ax.clear()
        
        # í™˜ê²½ ì „ì²´ ë°°ê²½
        self.ax.fill_between([0, self.corridor_width], 0, self.corridor_height, 
                            color='lightblue', alpha=0.2, label='ë³µë„')
        
        # ë³‘ëª© êµ¬ì—­ í‘œì‹œ (íšŒìƒ‰ ë²½ë“¤)
        center_y = self.corridor_height / 2
        bottleneck_x = self.bottleneck_position
        
        # ìœ„ìª½ ë²½
        upper_wall = patches.Rectangle(
            (bottleneck_x - 0.5, center_y + self.bottleneck_width/2), 
            1.0, self.corridor_height - (center_y + self.bottleneck_width/2),
            facecolor='darkgray', edgecolor='black', linewidth=2
        )
        self.ax.add_patch(upper_wall)
        
        # ì•„ë˜ìª½ ë²½  
        lower_wall = patches.Rectangle(
            (bottleneck_x - 0.5, 0), 
            1.0, center_y - self.bottleneck_width/2,
            facecolor='darkgray', edgecolor='black', linewidth=2
        )
        self.ax.add_patch(lower_wall)
        
        # ë³‘ëª© í†µë¡œ í‘œì‹œ (ë…¸ë€ìƒ‰ìœ¼ë¡œ ê°•ì¡°)
        bottleneck_passage = patches.Rectangle(
            (bottleneck_x - 0.5, center_y - self.bottleneck_width/2),
            1.0, self.bottleneck_width,
            facecolor='yellow', alpha=0.3, edgecolor='orange', linewidth=2
        )
        self.ax.add_patch(bottleneck_passage)
        
        # í™˜ê²½ ê²½ê³„ í…Œë‘ë¦¬
        boundary = patches.Rectangle(
            (0, 0), self.corridor_width, self.corridor_height,
            linewidth=3, edgecolor='black', facecolor='none'
        )
        self.ax.add_patch(boundary)
        
        # ë³‘ëª© ì¥ì• ë¬¼ ê·¸ë¦¬ê¸° (ì›í˜• ì¥ì• ë¬¼ë“¤)
        for obstacle in self.obstacles:
            obs_circle = patches.Circle(
                (obstacle.x, obstacle.y), obstacle.radius,
                color='red', alpha=0.9, edgecolor='darkred', linewidth=2
            )
            self.ax.add_patch(obs_circle)
        
        # ëª©í‘œ ì§€ì  ê·¸ë¦¬ê¸°
        for i, landmark in enumerate(self.landmarks):
            goal_circle = patches.Circle(
                (landmark.x, landmark.y), landmark.radius,
                color='green', alpha=0.7, edgecolor='darkgreen', linewidth=2
            )
            self.ax.add_patch(goal_circle)
            self.ax.text(landmark.x, landmark.y, f'G{i}', 
                        ha='center', va='center', fontweight='bold', fontsize=10)
        
        # ì—ì´ì „íŠ¸ ê·¸ë¦¬ê¸°
        colors = ['blue', 'orange', 'purple', 'brown', 'pink', 'cyan']
        for i, agent in enumerate(self.agents):
            color = colors[i % len(colors)]
            
            # ì—ì´ì „íŠ¸ ì›
            agent_circle = patches.Circle(
                (agent.x, agent.y), agent.radius,
                color=color, alpha=0.8, edgecolor='black', linewidth=1
            )
            self.ax.add_patch(agent_circle)
            
            # ì—ì´ì „íŠ¸ ID
            self.ax.text(agent.x, agent.y, str(i), 
                        ha='center', va='center', fontweight='bold', 
                        color='white', fontsize=8)
            
            # ëª©í‘œê¹Œì§€ì˜ ì„ 
            target = self.landmarks[agent.target_id]
            self.ax.plot([agent.x, target.x], [agent.y, target.y], 
                        color=color, alpha=0.5, linestyle='--', linewidth=1.5)
            
            # ì†ë„ ë²¡í„° (ë” ëª…í™•í•˜ê²Œ)
            speed = math.sqrt(agent.vx**2 + agent.vy**2)
            if speed > 0.1:
                scale = 3.0  # í™”ì‚´í‘œ í¬ê¸° ì¡°ì •
                self.ax.arrow(agent.x, agent.y, agent.vx*scale, agent.vy*scale,
                            head_width=0.15, head_length=0.15, 
                            fc=color, ec=color, alpha=0.8, linewidth=2)
        
        # ì„¤ì •
        self.ax.set_xlim(-0.5, self.corridor_width + 0.5)
        self.ax.set_ylim(-0.5, self.corridor_height + 0.5)
        self.ax.set_aspect('equal')
        self.ax.set_title(f'InforMARL 2D Bottleneck - Step {self.timestep}\\nì„±ê³µ: {self.success_count}, ì¶©ëŒ: {self.collision_count}', 
                         fontsize=12, fontweight='bold')
        self.ax.grid(True, alpha=0.3)
        
        # ë²”ë¡€ ì¶”ê°€
        legend_elements = [
            patches.Patch(color='darkgray', label='ë²½'),
            patches.Patch(color='yellow', alpha=0.3, label='ë³‘ëª© í†µë¡œ'),
            patches.Patch(color='green', label='ëª©í‘œ'),
            patches.Patch(color='red', label='ì¥ì• ë¬¼')
        ]
        self.ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))
        
        # ê°•ì œë¡œ í™”ë©´ ì—…ë°ì´íŠ¸
        plt.draw()
        plt.pause(0.01)
        
        if mode == 'human':
            plt.show(block=False)
    
    def evaluate_with_animation(self, num_episodes: int = 5, render_delay: float = 0.2):
        """í‰ê°€ ëª¨ë“œë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰í•˜ë©° ì• ë‹ˆë©”ì´ì…˜ í‘œì‹œ"""
        print("=== InforMARL í‰ê°€ ëª¨ë“œ (ì• ë‹ˆë©”ì´ì…˜) ===")
        print("ì°½ì´ ì—´ë¦¬ë©´ ì—ì´ì „íŠ¸ ì›€ì§ì„ì„ ê´€ì°°í•˜ì„¸ìš”!")
        
        episode_rewards = []
        
        for episode in range(num_episodes):
            print(f"\nì—í”¼ì†Œë“œ {episode + 1}/{num_episodes} ì‹œì‘")
            observations = self.reset()
            episode_reward = 0
            
            # ì´ˆê¸° ìƒíƒœ ë Œë”ë§
            self.render()
            time.sleep(render_delay * 2)  # ì´ˆê¸° ìƒíƒœ ì¢€ ë” ì˜¤ë˜ ë³´ì—¬ì£¼ê¸°
            
            for step in range(self.max_timesteps):
                # í‰ê°€ ëª¨ë“œë¡œ í–‰ë™ ì„ íƒ (training=False)
                actions, _, _ = self._get_batch_actions(
                    self._get_graph_observations(), training=False
                )
                
                # í•œ ìŠ¤í… ì‹¤í–‰
                observations, rewards, done, info = self.step(actions)
                episode_reward += sum(rewards)
                
                # ë Œë”ë§
                self.render()
                
                # ì›€ì§ì„ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê·¸ ì¶œë ¥
                if step % 20 == 0:
                    print(f"    ìŠ¤í… {step}: ì—ì´ì „íŠ¸ ìœ„ì¹˜ë“¤")
                    for i, agent in enumerate(self.agents):
                        print(f"      Agent {i}: ({agent.x:.1f}, {agent.y:.1f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
                
                time.sleep(render_delay)
                
                if done:
                    print(f"  ì—í”¼ì†Œë“œ ì™„ë£Œ! ìŠ¤í…: {step + 1}")
                    break
            
            episode_rewards.append(episode_reward)
            print(f"  ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.2f}")
            print(f"  ì„±ê³µë¥ : {info['success_rate']:.2f}")
            print(f"  ì¶©ëŒ íšŸìˆ˜: {info['collision_count']}")
            
            # ì—í”¼ì†Œë“œ ê°„ ì ì‹œ ëŒ€ê¸°
            print("  ë‹¤ìŒ ì—í”¼ì†Œë“œê¹Œì§€ ì ì‹œ ëŒ€ê¸°...")
            time.sleep(2.0)
        
        avg_reward = np.mean(episode_rewards)
        print(f"\n=== í‰ê°€ ê²°ê³¼ ===")
        print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {avg_reward:.3f}")
        
        return episode_rewards


def run_informarl_experiment(num_episodes: int = 100, num_agents: int = 6):
    """InforMARL ì‹¤í—˜ ì‹¤í–‰"""
    print("=== InforMARL 2D ë³‘ëª© í™˜ê²½ í•™ìŠµ ì‹œì‘ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents)
    episode_rewards = env.train_agents(num_episodes=num_episodes)
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
    print(f"í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {np.mean(episode_rewards):.3f}")
    
    return episode_rewards, env


def run_animation_demo(num_agents: int = 4):
    """ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ì‹¤í–‰"""
    print("=== InforMARL ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents)
    
    # ê°„ë‹¨í•œ í•™ìŠµ (ì„ íƒì‚¬í•­)
    print("ê°„ë‹¨í•œ ì‚¬ì „ í•™ìŠµ ì¤‘...")
    env.train_agents(num_episodes=10)
    
    # ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ í‰ê°€
    print("\ní‰ê°€ ëª¨ë“œ ì• ë‹ˆë©”ì´ì…˜ ì‹œì‘!")
    results = env.evaluate_with_animation(num_episodes=3, render_delay=0.2)
    
    return results


def quick_movement_test(num_agents: int = 2):
    """ì—ì´ì „íŠ¸ ì›€ì§ì„ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print("=== ì—ì´ì „íŠ¸ ì›€ì§ì„ í…ŒìŠ¤íŠ¸ ===")
    
    env = BottleneckInforMARLEnv(num_agents=num_agents, max_timesteps=50)
    observations = env.reset()
    
    print("ì´ˆê¸° ìƒíƒœ ë Œë”ë§...")
    env.render()
    time.sleep(1)
    
    for step in range(20):
        # ëœë¤ í–‰ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        random_actions = [np.random.randint(0, 4) for _ in range(num_agents)]
        
        print(f"ìŠ¤í… {step}: í–‰ë™ {random_actions}")
        observations, rewards, done, info = env.step(random_actions)
        
        # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ì¶œë ¥
        for i, agent in enumerate(env.agents):
            print(f"  Agent {i}: ({agent.x:.2f}, {agent.y:.2f}) ì†ë„: ({agent.vx:.2f}, {agent.vy:.2f})")
        
        env.render()
        time.sleep(0.5)
        
        if done:
            break
    
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    plt.show()
    return True


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "demo":
            # ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ì‹¤í–‰
            run_animation_demo(num_agents=4)
        elif sys.argv[1] == "test":
            # ë¹ ë¥¸ ì›€ì§ì„ í…ŒìŠ¤íŠ¸
            quick_movement_test(num_agents=2)
        else:
            print("ì‚¬ìš©ë²•: python bottleneck_informarl_v2.py [demo|test]")
    else:
        # ì¼ë°˜ í•™ìŠµ ì‹¤í–‰
        results, env = run_informarl_experiment(num_episodes=100, num_agents=4)
        
        # í•™ìŠµ í›„ ì• ë‹ˆë©”ì´ì…˜ ë³´ê¸°
        print("\ní•™ìŠµ ì™„ë£Œ! ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ ê²°ê³¼ í™•ì¸ (y/n)?")
        if input().lower() == 'y':
            env.evaluate_with_animation(num_episodes=2, render_delay=0.15)